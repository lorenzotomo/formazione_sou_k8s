
==> Audit <==
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    COMMAND     ‚îÇ                                                 ARGS                                                 ‚îÇ PROFILE  ‚îÇ    USER     ‚îÇ VERSION ‚îÇ     START TIME      ‚îÇ      END TIME       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ start          ‚îÇ                                                                                                      ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 31 Oct 25 11:39 CET ‚îÇ 31 Oct 25 11:43 CET ‚îÇ
‚îÇ delete         ‚îÇ                                                                                                      ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 31 Oct 25 11:51 CET ‚îÇ 31 Oct 25 11:51 CET ‚îÇ
‚îÇ start          ‚îÇ --driver=docker                                                                                      ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 31 Oct 25 11:51 CET ‚îÇ                     ‚îÇ
‚îÇ start          ‚îÇ --driver=docker                                                                                      ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 31 Oct 25 11:53 CET ‚îÇ 31 Oct 25 11:56 CET ‚îÇ
‚îÇ start          ‚îÇ                                                                                                      ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 07 Nov 25 16:24 CET ‚îÇ 07 Nov 25 16:25 CET ‚îÇ
‚îÇ update-context ‚îÇ                                                                                                      ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 09:41 CET ‚îÇ 10 Nov 25 09:41 CET ‚îÇ
‚îÇ stop           ‚îÇ                                                                                                      ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 09:42 CET ‚îÇ 10 Nov 25 09:42 CET ‚îÇ
‚îÇ start          ‚îÇ                                                                                                      ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 09:42 CET ‚îÇ 10 Nov 25 09:43 CET ‚îÇ
‚îÇ stop           ‚îÇ                                                                                                      ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 12:27 CET ‚îÇ 10 Nov 25 12:27 CET ‚îÇ
‚îÇ start          ‚îÇ --listen-address=0.0.0.0 --apiserver-ips=192.168.2.197                                               ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 12:27 CET ‚îÇ 10 Nov 25 12:28 CET ‚îÇ
‚îÇ delete         ‚îÇ                                                                                                      ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 12:32 CET ‚îÇ 10 Nov 25 12:32 CET ‚îÇ
‚îÇ start          ‚îÇ --driver=hyperkit --apiserver-ips=192.168.2.197                                                      ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 12:33 CET ‚îÇ                     ‚îÇ
‚îÇ delete         ‚îÇ                                                                                                      ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 12:39 CET ‚îÇ 10 Nov 25 12:39 CET ‚îÇ
‚îÇ start          ‚îÇ --driver=virtualbox --apiserver-ips=192.168.2.197                                                    ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 12:39 CET ‚îÇ                     ‚îÇ
‚îÇ delete         ‚îÇ                                                                                                      ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 12:43 CET ‚îÇ 10 Nov 25 12:43 CET ‚îÇ
‚îÇ start          ‚îÇ --driver=vfkit --apiserver-ips=192.168.2.197 --listen-address=0.0.0.0                                ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 12:44 CET ‚îÇ                     ‚îÇ
‚îÇ delete         ‚îÇ --all                                                                                                ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 12:57 CET ‚îÇ 10 Nov 25 12:57 CET ‚îÇ
‚îÇ delete         ‚îÇ --all                                                                                                ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 12:57 CET ‚îÇ 10 Nov 25 12:57 CET ‚îÇ
‚îÇ delete         ‚îÇ --all                                                                                                ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 12:58 CET ‚îÇ 10 Nov 25 12:58 CET ‚îÇ
‚îÇ start          ‚îÇ --driver=vfkit --memory=2048 --cpus=2 --apiserver-ips=192.168.2.197 --listen-address=0.0.0.0         ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 14:12 CET ‚îÇ                     ‚îÇ
‚îÇ delete         ‚îÇ --all                                                                                                ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 14:19 CET ‚îÇ 10 Nov 25 14:19 CET ‚îÇ
‚îÇ start          ‚îÇ --driver=hyperkit --apiserver-ips=192.168.2.197                                                      ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 14:20 CET ‚îÇ                     ‚îÇ
‚îÇ delete         ‚îÇ --all                                                                                                ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 14:20 CET ‚îÇ 10 Nov 25 14:20 CET ‚îÇ
‚îÇ start          ‚îÇ --driver=docker --apiserver-ips=192.168.2.197                                                        ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 14:21 CET ‚îÇ 10 Nov 25 14:22 CET ‚îÇ
‚îÇ ip             ‚îÇ                                                                                                      ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 14:23 CET ‚îÇ 10 Nov 25 14:23 CET ‚îÇ
‚îÇ delete         ‚îÇ                                                                                                      ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 14:50 CET ‚îÇ 10 Nov 25 14:50 CET ‚îÇ
‚îÇ delete         ‚îÇ --all                                                                                                ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 14:54 CET ‚îÇ 10 Nov 25 14:54 CET ‚îÇ
‚îÇ start          ‚îÇ --driver=docker --listen-address=0.0.0.0 --apiserver-ips=192.168.3.1 --apiserver-name=minikube.local ‚îÇ minikube ‚îÇ lorenzotomo ‚îÇ v1.37.0 ‚îÇ 10 Nov 25 14:54 CET ‚îÇ                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


==> Last Start <==
Log file created at: 2025/11/10 14:54:17
Running on machine: MacBook-Pro-di-Lorenzo
Binary: Built with gc go1.24.6 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1110 14:54:17.339678   17843 out.go:360] Setting OutFile to fd 1 ...
I1110 14:54:17.339918   17843 out.go:413] isatty.IsTerminal(1) = true
I1110 14:54:17.339922   17843 out.go:374] Setting ErrFile to fd 2...
I1110 14:54:17.339927   17843 out.go:413] isatty.IsTerminal(2) = true
I1110 14:54:17.340136   17843 root.go:338] Updating PATH: /Users/lorenzotomo/.minikube/bin
I1110 14:54:17.341256   17843 out.go:368] Setting JSON to false
I1110 14:54:17.391649   17843 start.go:130] hostinfo: {"hostname":"MacBook-Pro-di-Lorenzo.local","uptime":21417,"bootTime":1762761440,"procs":434,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"13.7.8","kernelVersion":"22.6.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"4aea8a9b-2801-5d49-aee7-af2e7f4f6a19"}
W1110 14:54:17.391866   17843 start.go:138] gopshost.Virtualization returned error: not implemented yet
I1110 14:54:17.404805   17843 out.go:179] üòÑ  minikube v1.37.0 on Darwin 13.7.8
I1110 14:54:17.434513   17843 notify.go:220] Checking for updates...
I1110 14:54:17.436756   17843 driver.go:421] Setting default libvirt URI to qemu:///system
I1110 14:54:17.576495   17843 docker.go:123] docker version: linux-28.5.1:Docker Desktop 4.48.0 (207573)
I1110 14:54:17.577070   17843 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1110 14:54:19.111572   17843 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.534335501s)
I1110 14:54:19.113289   17843 info.go:266] docker info: {ID:a47f8252-cd06-471e-8ce6-d923b48a3181 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:4 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:44 OomKillDisable:false NGoroutines:100 SystemTime:2025-11-10 13:54:19.021491432 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.10.14-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4104384512 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/lorenzotomo/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/lorenzotomo/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:/Users/lorenzotomo/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1-desktop.1] map[Name:cloud Path:/Users/lorenzotomo/.docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.39] map[Name:compose Path:/Users/lorenzotomo/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.2-desktop.1] map[Name:debug Path:/Users/lorenzotomo/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.44] map[Name:desktop Path:/Users/lorenzotomo/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:/Users/lorenzotomo/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:/Users/lorenzotomo/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/Users/lorenzotomo/.docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.24.0] map[Name:sbom Path:/Users/lorenzotomo/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/lorenzotomo/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I1110 14:54:19.122974   17843 out.go:179] ‚ú®  Using the docker driver based on user configuration
I1110 14:54:19.150834   17843 start.go:304] selected driver: docker
I1110 14:54:19.150864   17843 start.go:918] validating driver "docker" against <nil>
I1110 14:54:19.150898   17843 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1110 14:54:19.151341   17843 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1110 14:54:19.518669   17843 info.go:266] docker info: {ID:a47f8252-cd06-471e-8ce6-d923b48a3181 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:4 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:44 OomKillDisable:false NGoroutines:100 SystemTime:2025-11-10 13:54:19.485303558 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:6.10.14-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4104384512 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/lorenzotomo/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/lorenzotomo/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.11] map[Name:buildx Path:/Users/lorenzotomo/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.29.1-desktop.1] map[Name:cloud Path:/Users/lorenzotomo/.docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.39] map[Name:compose Path:/Users/lorenzotomo/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.2-desktop.1] map[Name:debug Path:/Users/lorenzotomo/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.44] map[Name:desktop Path:/Users/lorenzotomo/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.2.0] map[Name:extension Path:/Users/lorenzotomo/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.31] map[Name:init Path:/Users/lorenzotomo/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/Users/lorenzotomo/.docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.24.0] map[Name:sbom Path:/Users/lorenzotomo/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/lorenzotomo/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.3]] Warnings:<nil>}}
I1110 14:54:19.519014   17843 start_flags.go:327] no existing cluster config was found, will generate one from the flags 
I1110 14:54:19.525544   17843 start_flags.go:410] Using suggested 3072MB memory alloc based on sys=8192MB, container=3914MB
I1110 14:54:19.526372   17843 start_flags.go:974] Wait components to verify : map[apiserver:true system_pods:true]
I1110 14:54:19.537067   17843 out.go:179] üìå  Using Docker Desktop driver with root privileges
I1110 14:54:19.545825   17843 cni.go:84] Creating CNI manager for ""
I1110 14:54:19.545978   17843 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1110 14:54:19.546005   17843 start_flags.go:336] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I1110 14:54:19.546307   17843 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikube.local APIServerNames:[] APIServerIPs:[192.168.3.1] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress:0.0.0.0 Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1110 14:54:19.571189   17843 out.go:179] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I1110 14:54:19.583483   17843 cache.go:123] Beginning downloading kic base image for docker with docker
I1110 14:54:19.596991   17843 out.go:179] üöú  Pulling base image v0.0.48 ...
I1110 14:54:19.609378   17843 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1110 14:54:19.609468   17843 preload.go:146] Found local preload: /Users/lorenzotomo/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1110 14:54:19.609480   17843 cache.go:58] Caching tarball of preloaded images
I1110 14:54:19.609824   17843 preload.go:172] Found /Users/lorenzotomo/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1110 14:54:19.609840   17843 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I1110 14:54:19.609833   17843 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I1110 14:54:19.611880   17843 profile.go:143] Saving config to /Users/lorenzotomo/.minikube/profiles/minikube/config.json ...
I1110 14:54:19.612074   17843 lock.go:35] WriteFile acquiring /Users/lorenzotomo/.minikube/profiles/minikube/config.json: {Name:mk776e1d65bd0f0137535175811c7c6c906b60ab Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1110 14:54:19.721204   17843 cache.go:152] Downloading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I1110 14:54:19.721495   17843 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory
I1110 14:54:19.721556   17843 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory, skipping pull
I1110 14:54:19.721569   17843 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 exists in cache, skipping pull
I1110 14:54:19.721595   17843 cache.go:155] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 as a tarball
I1110 14:54:19.721601   17843 cache.go:165] Loading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from local cache
I1110 14:54:40.345346   17843 cache.go:167] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from cached tarball
I1110 14:54:40.346724   17843 cache.go:232] Successfully downloaded all kic artifacts
I1110 14:54:40.349784   17843 start.go:360] acquireMachinesLock for minikube: {Name:mk5048267b22865c3b711d2a0383cc601ebc663c Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1110 14:54:40.350367   17843 start.go:364] duration metric: took 369.992¬µs to acquireMachinesLock for "minikube"
I1110 14:54:40.351546   17843 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikube.local APIServerNames:[] APIServerIPs:[192.168.3.1] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress:0.0.0.0 Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1110 14:54:40.352068   17843 start.go:125] createHost starting for "" (driver="docker")
I1110 14:54:40.367792   17843 out.go:252] üî•  Creating docker container (CPUs=2, Memory=3072MB) ...
I1110 14:54:40.379641   17843 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1110 14:54:40.379738   17843 client.go:168] LocalClient.Create starting
I1110 14:54:40.381148   17843 main.go:141] libmachine: Reading certificate data from /Users/lorenzotomo/.minikube/certs/ca.pem
I1110 14:54:40.381654   17843 main.go:141] libmachine: Decoding PEM data...
I1110 14:54:40.382308   17843 main.go:141] libmachine: Parsing certificate...
I1110 14:54:40.385999   17843 main.go:141] libmachine: Reading certificate data from /Users/lorenzotomo/.minikube/certs/cert.pem
I1110 14:54:40.388286   17843 main.go:141] libmachine: Decoding PEM data...
I1110 14:54:40.388308   17843 main.go:141] libmachine: Parsing certificate...
I1110 14:54:40.391197   17843 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1110 14:54:40.455388   17843 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1110 14:54:40.455815   17843 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I1110 14:54:40.455844   17843 cli_runner.go:164] Run: docker network inspect minikube
W1110 14:54:40.489548   17843 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1110 14:54:40.489619   17843 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I1110 14:54:40.489642   17843 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I1110 14:54:40.489937   17843 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1110 14:54:40.524949   17843 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001527220}
I1110 14:54:40.525383   17843 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 65535 ...
I1110 14:54:40.525532   17843 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=65535 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1110 14:54:40.633784   17843 network_create.go:108] docker network minikube 192.168.49.0/24 created
I1110 14:54:40.634235   17843 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I1110 14:54:40.654574   17843 out.go:179] üí°  minikube is not meant for production use. You are opening non-local traffic
W1110 14:54:40.674813   17843 out.go:285] ‚ùó  Listening to 0.0.0.0. This is not recommended and can cause a security vulnerability. Use at your own risk
I1110 14:54:40.679123   17843 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1110 14:54:40.738976   17843 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1110 14:54:40.777644   17843 oci.go:103] Successfully created a docker volume minikube
I1110 14:54:40.777985   17843 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -d /var/lib
I1110 14:54:41.671910   17843 oci.go:107] Successfully prepared a docker volume minikube
I1110 14:54:41.672295   17843 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1110 14:54:41.672397   17843 kic.go:194] Starting extracting preloaded images to volume ...
I1110 14:54:41.672596   17843 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/lorenzotomo/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir
I1110 14:54:51.781771   17843 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/lorenzotomo/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir: (10.105034673s)
I1110 14:54:51.784161   17843 kic.go:203] duration metric: took 10.11056175s to extract preloaded images to volume ...
I1110 14:54:51.789752   17843 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1110 14:54:54.065984   17843 cli_runner.go:217] Completed: docker info --format "'{{json .SecurityOptions}}'": (2.276055252s)
I1110 14:54:54.068480   17843 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3072mb --memory-swap=3072mb --cpus=2 -e container=docker --expose 8443 --publish=0.0.0.0::8443 --publish=0.0.0.0::22 --publish=0.0.0.0::2376 --publish=0.0.0.0::5000 --publish=0.0.0.0::32443 gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1
I1110 14:54:54.660228   17843 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1110 14:54:54.733462   17843 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1110 14:54:54.857922   17843 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1110 14:54:55.076413   17843 oci.go:144] the created container "minikube" has a running status.
I1110 14:54:55.079511   17843 kic.go:225] Creating ssh key for kic: /Users/lorenzotomo/.minikube/machines/minikube/id_rsa...
I1110 14:54:55.411991   17843 kic_runner.go:191] docker (temp): /Users/lorenzotomo/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1110 14:54:55.513666   17843 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1110 14:54:55.596528   17843 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1110 14:54:55.596547   17843 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1110 14:54:56.024588   17843 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1110 14:54:56.069789   17843 machine.go:93] provisionDockerMachine start ...
I1110 14:54:56.071539   17843 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 14:54:56.118942   17843 main.go:141] libmachine: Using SSH client type: native
I1110 14:54:56.125135   17843 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xac22480] 0xac25180 <nil>  [] 0s} 127.0.0.1 50356 <nil> <nil>}
I1110 14:54:56.125175   17843 main.go:141] libmachine: About to run SSH command:
hostname
I1110 14:54:56.537827   17843 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1110 14:54:56.539284   17843 ubuntu.go:182] provisioning hostname "minikube"
I1110 14:54:56.540297   17843 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 14:54:56.587855   17843 main.go:141] libmachine: Using SSH client type: native
I1110 14:54:56.588176   17843 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xac22480] 0xac25180 <nil>  [] 0s} 127.0.0.1 50356 <nil> <nil>}
I1110 14:54:56.588183   17843 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1110 14:54:57.009961   17843 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1110 14:54:57.010517   17843 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 14:54:57.091937   17843 main.go:141] libmachine: Using SSH client type: native
I1110 14:54:57.092528   17843 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xac22480] 0xac25180 <nil>  [] 0s} 127.0.0.1 50356 <nil> <nil>}
I1110 14:54:57.092544   17843 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1110 14:54:57.283496   17843 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1110 14:54:57.283572   17843 ubuntu.go:188] set auth options {CertDir:/Users/lorenzotomo/.minikube CaCertPath:/Users/lorenzotomo/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/lorenzotomo/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/lorenzotomo/.minikube/machines/server.pem ServerKeyPath:/Users/lorenzotomo/.minikube/machines/server-key.pem ClientKeyPath:/Users/lorenzotomo/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/lorenzotomo/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/lorenzotomo/.minikube}
I1110 14:54:57.283961   17843 ubuntu.go:190] setting up certificates
I1110 14:54:57.283977   17843 provision.go:84] configureAuth start
I1110 14:54:57.284420   17843 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1110 14:54:57.334783   17843 provision.go:143] copyHostCerts
I1110 14:54:57.335294   17843 exec_runner.go:144] found /Users/lorenzotomo/.minikube/ca.pem, removing ...
I1110 14:54:57.335308   17843 exec_runner.go:203] rm: /Users/lorenzotomo/.minikube/ca.pem
I1110 14:54:57.336002   17843 exec_runner.go:151] cp: /Users/lorenzotomo/.minikube/certs/ca.pem --> /Users/lorenzotomo/.minikube/ca.pem (1090 bytes)
I1110 14:54:57.337055   17843 exec_runner.go:144] found /Users/lorenzotomo/.minikube/cert.pem, removing ...
I1110 14:54:57.337065   17843 exec_runner.go:203] rm: /Users/lorenzotomo/.minikube/cert.pem
I1110 14:54:57.337273   17843 exec_runner.go:151] cp: /Users/lorenzotomo/.minikube/certs/cert.pem --> /Users/lorenzotomo/.minikube/cert.pem (1135 bytes)
I1110 14:54:57.338088   17843 exec_runner.go:144] found /Users/lorenzotomo/.minikube/key.pem, removing ...
I1110 14:54:57.338120   17843 exec_runner.go:203] rm: /Users/lorenzotomo/.minikube/key.pem
I1110 14:54:57.338251   17843 exec_runner.go:151] cp: /Users/lorenzotomo/.minikube/certs/key.pem --> /Users/lorenzotomo/.minikube/key.pem (1675 bytes)
I1110 14:54:57.338821   17843 provision.go:117] generating server cert: /Users/lorenzotomo/.minikube/machines/server.pem ca-key=/Users/lorenzotomo/.minikube/certs/ca.pem private-key=/Users/lorenzotomo/.minikube/certs/ca-key.pem org=lorenzotomo.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1110 14:54:57.893882   17843 provision.go:177] copyRemoteCerts
I1110 14:54:57.894910   17843 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1110 14:54:57.895051   17843 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 14:54:57.981089   17843 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50356 SSHKeyPath:/Users/lorenzotomo/.minikube/machines/minikube/id_rsa Username:docker}
I1110 14:54:58.084635   17843 ssh_runner.go:362] scp /Users/lorenzotomo/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1090 bytes)
I1110 14:54:58.128029   17843 ssh_runner.go:362] scp /Users/lorenzotomo/.minikube/machines/server.pem --> /etc/docker/server.pem (1192 bytes)
I1110 14:54:58.158715   17843 ssh_runner.go:362] scp /Users/lorenzotomo/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1110 14:54:58.200373   17843 provision.go:87] duration metric: took 915.701967ms to configureAuth
I1110 14:54:58.200387   17843 ubuntu.go:206] setting minikube options for container-runtime
I1110 14:54:58.201979   17843 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1110 14:54:58.202087   17843 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 14:54:58.242997   17843 main.go:141] libmachine: Using SSH client type: native
I1110 14:54:58.243266   17843 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xac22480] 0xac25180 <nil>  [] 0s} 127.0.0.1 50356 <nil> <nil>}
I1110 14:54:58.243273   17843 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1110 14:54:58.367683   17843 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1110 14:54:58.367704   17843 ubuntu.go:71] root file system type: overlay
I1110 14:54:58.369119   17843 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1110 14:54:58.369582   17843 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 14:54:58.413288   17843 main.go:141] libmachine: Using SSH client type: native
I1110 14:54:58.413627   17843 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xac22480] 0xac25180 <nil>  [] 0s} 127.0.0.1 50356 <nil> <nil>}
I1110 14:54:58.413736   17843 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1110 14:54:58.557477   17843 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I1110 14:54:58.557965   17843 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 14:54:58.592979   17843 main.go:141] libmachine: Using SSH client type: native
I1110 14:54:58.593222   17843 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xac22480] 0xac25180 <nil>  [] 0s} 127.0.0.1 50356 <nil> <nil>}
I1110 14:54:58.593233   17843 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1110 14:55:00.994907   17843 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-09-03 20:55:49.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-11-10 13:54:58.551473503 +0000
@@ -9,23 +9,34 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1110 14:55:00.994929   17843 machine.go:96] duration metric: took 4.925018064s to provisionDockerMachine
I1110 14:55:00.994942   17843 client.go:171] duration metric: took 20.61480347s to LocalClient.Create
I1110 14:55:00.994987   17843 start.go:167] duration metric: took 20.614952364s to libmachine.API.Create "minikube"
I1110 14:55:00.995598   17843 start.go:293] postStartSetup for "minikube" (driver="docker")
I1110 14:55:00.995624   17843 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1110 14:55:00.996199   17843 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1110 14:55:00.996264   17843 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 14:55:01.046961   17843 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50356 SSHKeyPath:/Users/lorenzotomo/.minikube/machines/minikube/id_rsa Username:docker}
I1110 14:55:01.238426   17843 ssh_runner.go:195] Run: cat /etc/os-release
I1110 14:55:01.247706   17843 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1110 14:55:01.247739   17843 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1110 14:55:01.247754   17843 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1110 14:55:01.247830   17843 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1110 14:55:01.248109   17843 filesync.go:126] Scanning /Users/lorenzotomo/.minikube/addons for local assets ...
I1110 14:55:01.248535   17843 filesync.go:126] Scanning /Users/lorenzotomo/.minikube/files for local assets ...
I1110 14:55:01.248601   17843 start.go:296] duration metric: took 252.98714ms for postStartSetup
I1110 14:55:01.249584   17843 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1110 14:55:01.289541   17843 profile.go:143] Saving config to /Users/lorenzotomo/.minikube/profiles/minikube/config.json ...
I1110 14:55:01.290951   17843 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1110 14:55:01.291032   17843 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 14:55:01.329434   17843 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50356 SSHKeyPath:/Users/lorenzotomo/.minikube/machines/minikube/id_rsa Username:docker}
I1110 14:55:01.400751   17843 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1110 14:55:01.410885   17843 start.go:128] duration metric: took 21.057705771s to createHost
I1110 14:55:01.410993   17843 start.go:83] releasing machines lock for "minikube", held for 21.0602103s
I1110 14:55:01.411482   17843 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1110 14:55:01.452674   17843 ssh_runner.go:195] Run: cat /version.json
I1110 14:55:01.452807   17843 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 14:55:01.454619   17843 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1110 14:55:01.455647   17843 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 14:55:01.506071   17843 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50356 SSHKeyPath:/Users/lorenzotomo/.minikube/machines/minikube/id_rsa Username:docker}
I1110 14:55:01.506157   17843 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50356 SSHKeyPath:/Users/lorenzotomo/.minikube/machines/minikube/id_rsa Username:docker}
I1110 14:55:01.818491   17843 ssh_runner.go:195] Run: systemctl --version
I1110 14:55:01.827119   17843 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1110 14:55:01.835688   17843 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1110 14:55:01.880966   17843 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1110 14:55:01.881100   17843 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1110 14:55:01.919567   17843 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I1110 14:55:01.919580   17843 start.go:495] detecting cgroup driver to use...
I1110 14:55:01.919623   17843 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1110 14:55:01.921786   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1110 14:55:01.950325   17843 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I1110 14:55:01.967902   17843 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1110 14:55:01.984924   17843 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1110 14:55:01.985038   17843 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1110 14:55:02.000872   17843 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1110 14:55:02.015457   17843 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1110 14:55:02.030610   17843 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1110 14:55:02.043707   17843 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1110 14:55:02.056551   17843 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1110 14:55:02.071725   17843 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1110 14:55:02.086728   17843 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1110 14:55:02.102914   17843 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1110 14:55:02.116852   17843 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1110 14:55:02.132867   17843 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1110 14:55:02.239329   17843 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1110 14:55:02.360972   17843 start.go:495] detecting cgroup driver to use...
I1110 14:55:02.360998   17843 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1110 14:55:02.361549   17843 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1110 14:55:02.384537   17843 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1110 14:55:02.401527   17843 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1110 14:55:02.427587   17843 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1110 14:55:02.445701   17843 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1110 14:55:02.462665   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1110 14:55:02.488637   17843 ssh_runner.go:195] Run: which cri-dockerd
I1110 14:55:02.498988   17843 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1110 14:55:02.514825   17843 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I1110 14:55:02.541337   17843 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1110 14:55:02.636117   17843 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1110 14:55:02.715796   17843 docker.go:575] configuring docker to use "cgroupfs" as cgroup driver...
I1110 14:55:02.717860   17843 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1110 14:55:02.740205   17843 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I1110 14:55:02.754999   17843 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1110 14:55:02.839441   17843 ssh_runner.go:195] Run: sudo systemctl restart docker
I1110 14:55:04.225628   17843 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.386136808s)
I1110 14:55:04.225767   17843 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1110 14:55:04.240443   17843 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1110 14:55:04.257315   17843 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1110 14:55:04.274014   17843 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1110 14:55:04.361583   17843 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1110 14:55:04.469363   17843 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1110 14:55:04.571872   17843 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1110 14:55:04.613423   17843 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I1110 14:55:04.631854   17843 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1110 14:55:04.726409   17843 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1110 14:55:05.288267   17843 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1110 14:55:05.314101   17843 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1110 14:55:05.315040   17843 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1110 14:55:05.324517   17843 start.go:563] Will wait 60s for crictl version
I1110 14:55:05.324694   17843 ssh_runner.go:195] Run: which crictl
I1110 14:55:05.331746   17843 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1110 14:55:05.485001   17843 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I1110 14:55:05.485193   17843 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1110 14:55:05.591087   17843 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1110 14:55:05.649411   17843 out.go:252] üê≥  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I1110 14:55:05.655334   17843 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1110 14:55:05.854134   17843 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1110 14:55:05.855089   17843 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1110 14:55:05.861563   17843 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1110 14:55:05.879529   17843 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1110 14:55:05.940413   17843 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikube.local APIServerNames:[] APIServerIPs:[192.168.3.1] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress:0.0.0.0 Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1110 14:55:05.940962   17843 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1110 14:55:05.941086   17843 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1110 14:55:05.980137   17843 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1110 14:55:05.980157   17843 docker.go:621] Images already preloaded, skipping extraction
I1110 14:55:05.980583   17843 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1110 14:55:06.016287   17843 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1110 14:55:06.016636   17843 cache_images.go:85] Images are preloaded, skipping loading
I1110 14:55:06.016944   17843 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I1110 14:55:06.018557   17843 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikube.local APIServerNames:[] APIServerIPs:[192.168.3.1] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1110 14:55:06.018689   17843 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1110 14:55:06.395605   17843 cni.go:84] Creating CNI manager for ""
I1110 14:55:06.395629   17843 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1110 14:55:06.395964   17843 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1110 14:55:06.396016   17843 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1110 14:55:06.396708   17843 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1110 14:55:06.397127   17843 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I1110 14:55:06.414463   17843 binaries.go:44] Found k8s binaries, skipping transfer
I1110 14:55:06.414635   17843 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1110 14:55:06.431170   17843 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1110 14:55:06.458190   17843 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1110 14:55:06.486329   17843 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2209 bytes)
I1110 14:55:06.519715   17843 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1110 14:55:06.528229   17843 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1110 14:55:06.546228   17843 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1110 14:55:06.761203   17843 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1110 14:55:06.800895   17843 certs.go:68] Setting up /Users/lorenzotomo/.minikube/profiles/minikube for IP: 192.168.49.2
I1110 14:55:06.801231   17843 certs.go:194] generating shared ca certs ...
I1110 14:55:06.801926   17843 certs.go:226] acquiring lock for ca certs: {Name:mk8afe3796275d7aae9f755333723a03a9d87fe5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1110 14:55:06.803450   17843 certs.go:235] skipping valid "minikubeCA" ca cert: /Users/lorenzotomo/.minikube/ca.key
I1110 14:55:06.804038   17843 certs.go:235] skipping valid "proxyClientCA" ca cert: /Users/lorenzotomo/.minikube/proxy-client-ca.key
I1110 14:55:06.804058   17843 certs.go:256] generating profile certs ...
I1110 14:55:06.804166   17843 certs.go:363] generating signed profile cert for "minikube-user": /Users/lorenzotomo/.minikube/profiles/minikube/client.key
I1110 14:55:06.805376   17843 crypto.go:68] Generating cert /Users/lorenzotomo/.minikube/profiles/minikube/client.crt with IP's: []
I1110 14:55:07.041507   17843 crypto.go:156] Writing cert to /Users/lorenzotomo/.minikube/profiles/minikube/client.crt ...
I1110 14:55:07.041537   17843 lock.go:35] WriteFile acquiring /Users/lorenzotomo/.minikube/profiles/minikube/client.crt: {Name:mkdbfe9a22d7a3f9afd219d7551222f261e9c9ca Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1110 14:55:07.042013   17843 crypto.go:164] Writing key to /Users/lorenzotomo/.minikube/profiles/minikube/client.key ...
I1110 14:55:07.042029   17843 lock.go:35] WriteFile acquiring /Users/lorenzotomo/.minikube/profiles/minikube/client.key: {Name:mk3c99d08828e216aa4089146db43de174ed1a5a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1110 14:55:07.042504   17843 certs.go:363] generating signed profile cert for "minikube": /Users/lorenzotomo/.minikube/profiles/minikube/apiserver.key.1a70c5ab
I1110 14:55:07.042524   17843 crypto.go:68] Generating cert /Users/lorenzotomo/.minikube/profiles/minikube/apiserver.crt.1a70c5ab with IP's: [192.168.3.1 10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I1110 14:55:07.515812   17843 crypto.go:156] Writing cert to /Users/lorenzotomo/.minikube/profiles/minikube/apiserver.crt.1a70c5ab ...
I1110 14:55:07.515829   17843 lock.go:35] WriteFile acquiring /Users/lorenzotomo/.minikube/profiles/minikube/apiserver.crt.1a70c5ab: {Name:mk76c910b7ec55e261d35f9e626f50bc42c12c5d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1110 14:55:07.516415   17843 crypto.go:164] Writing key to /Users/lorenzotomo/.minikube/profiles/minikube/apiserver.key.1a70c5ab ...
I1110 14:55:07.516424   17843 lock.go:35] WriteFile acquiring /Users/lorenzotomo/.minikube/profiles/minikube/apiserver.key.1a70c5ab: {Name:mk5dfeb752b0eacb2c58ef06c2dcb34394d2dfe9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1110 14:55:07.516840   17843 certs.go:381] copying /Users/lorenzotomo/.minikube/profiles/minikube/apiserver.crt.1a70c5ab -> /Users/lorenzotomo/.minikube/profiles/minikube/apiserver.crt
I1110 14:55:07.535761   17843 certs.go:385] copying /Users/lorenzotomo/.minikube/profiles/minikube/apiserver.key.1a70c5ab -> /Users/lorenzotomo/.minikube/profiles/minikube/apiserver.key
I1110 14:55:07.536129   17843 certs.go:363] generating signed profile cert for "aggregator": /Users/lorenzotomo/.minikube/profiles/minikube/proxy-client.key
I1110 14:55:07.536149   17843 crypto.go:68] Generating cert /Users/lorenzotomo/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I1110 14:55:07.943142   17843 crypto.go:156] Writing cert to /Users/lorenzotomo/.minikube/profiles/minikube/proxy-client.crt ...
I1110 14:55:07.943154   17843 lock.go:35] WriteFile acquiring /Users/lorenzotomo/.minikube/profiles/minikube/proxy-client.crt: {Name:mk2ae4750f5235321f37c4bba142a5e5f7924842 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1110 14:55:07.943449   17843 crypto.go:164] Writing key to /Users/lorenzotomo/.minikube/profiles/minikube/proxy-client.key ...
I1110 14:55:07.943454   17843 lock.go:35] WriteFile acquiring /Users/lorenzotomo/.minikube/profiles/minikube/proxy-client.key: {Name:mk4d9641f1666021a9ce5e7509f10bd3a9e9d6cc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1110 14:55:07.945356   17843 certs.go:484] found cert: /Users/lorenzotomo/.minikube/certs/ca-key.pem (1675 bytes)
I1110 14:55:07.945887   17843 certs.go:484] found cert: /Users/lorenzotomo/.minikube/certs/ca.pem (1090 bytes)
I1110 14:55:07.945971   17843 certs.go:484] found cert: /Users/lorenzotomo/.minikube/certs/cert.pem (1135 bytes)
I1110 14:55:07.946468   17843 certs.go:484] found cert: /Users/lorenzotomo/.minikube/certs/key.pem (1675 bytes)
I1110 14:55:07.965620   17843 ssh_runner.go:362] scp /Users/lorenzotomo/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1110 14:55:07.998437   17843 ssh_runner.go:362] scp /Users/lorenzotomo/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1110 14:55:08.033466   17843 ssh_runner.go:362] scp /Users/lorenzotomo/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1110 14:55:08.063840   17843 ssh_runner.go:362] scp /Users/lorenzotomo/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1110 14:55:08.095602   17843 ssh_runner.go:362] scp /Users/lorenzotomo/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1428 bytes)
I1110 14:55:08.126509   17843 ssh_runner.go:362] scp /Users/lorenzotomo/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1110 14:55:08.160627   17843 ssh_runner.go:362] scp /Users/lorenzotomo/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1110 14:55:08.190942   17843 ssh_runner.go:362] scp /Users/lorenzotomo/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1110 14:55:08.224235   17843 ssh_runner.go:362] scp /Users/lorenzotomo/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1110 14:55:08.259855   17843 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1110 14:55:08.284084   17843 ssh_runner.go:195] Run: openssl version
I1110 14:55:08.310716   17843 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1110 14:55:08.334949   17843 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1110 14:55:08.340881   17843 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Oct 31 10:42 /usr/share/ca-certificates/minikubeCA.pem
I1110 14:55:08.341084   17843 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1110 14:55:08.351659   17843 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1110 14:55:08.364703   17843 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1110 14:55:08.371509   17843 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I1110 14:55:08.371567   17843 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikube.local APIServerNames:[] APIServerIPs:[192.168.3.1] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress:0.0.0.0 Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1110 14:55:08.371732   17843 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1110 14:55:08.400517   17843 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1110 14:55:08.412730   17843 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1110 14:55:08.428215   17843 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I1110 14:55:08.428297   17843 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1110 14:55:08.441731   17843 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1110 14:55:08.441764   17843 kubeadm.go:157] found existing configuration files:

I1110 14:55:08.441873   17843 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1110 14:55:08.454354   17843 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I1110 14:55:08.454474   17843 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I1110 14:55:08.465571   17843 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1110 14:55:08.476531   17843 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I1110 14:55:08.476656   17843 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I1110 14:55:08.488931   17843 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1110 14:55:08.500411   17843 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I1110 14:55:08.500506   17843 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1110 14:55:08.512843   17843 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1110 14:55:08.529280   17843 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I1110 14:55:08.529432   17843 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1110 14:55:08.544535   17843 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1110 14:55:08.665288   17843 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I1110 14:55:08.762926   17843 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1110 14:55:24.642082   17843 kubeadm.go:310] [init] Using Kubernetes version: v1.34.0
I1110 14:55:24.642177   17843 kubeadm.go:310] [preflight] Running pre-flight checks
I1110 14:55:24.642363   17843 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I1110 14:55:24.642496   17843 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1110 14:55:24.642614   17843 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I1110 14:55:24.642700   17843 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1110 14:55:24.655528   17843 out.go:252]     ‚ñ™ Generating certificates and keys ...
I1110 14:55:24.655754   17843 kubeadm.go:310] [certs] Using existing ca certificate authority
I1110 14:55:24.655905   17843 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I1110 14:55:24.655999   17843 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I1110 14:55:24.656070   17843 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I1110 14:55:24.656151   17843 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I1110 14:55:24.656239   17843 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I1110 14:55:24.656344   17843 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I1110 14:55:24.656434   17843 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1110 14:55:24.656475   17843 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I1110 14:55:24.656610   17843 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1110 14:55:24.656695   17843 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I1110 14:55:24.656786   17843 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I1110 14:55:24.656891   17843 kubeadm.go:310] [certs] Generating "sa" key and public key
I1110 14:55:24.656933   17843 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1110 14:55:24.656969   17843 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I1110 14:55:24.657009   17843 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I1110 14:55:24.657054   17843 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1110 14:55:24.657100   17843 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1110 14:55:24.657138   17843 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1110 14:55:24.657196   17843 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1110 14:55:24.657281   17843 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1110 14:55:24.667721   17843 out.go:252]     ‚ñ™ Booting up control plane ...
I1110 14:55:24.667824   17843 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1110 14:55:24.667908   17843 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1110 14:55:24.668018   17843 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1110 14:55:24.668161   17843 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1110 14:55:24.668297   17843 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/instance-config.yaml"
I1110 14:55:24.668518   17843 kubeadm.go:310] [patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
I1110 14:55:24.668651   17843 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1110 14:55:24.668698   17843 kubeadm.go:310] [kubelet-start] Starting the kubelet
I1110 14:55:24.668876   17843 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I1110 14:55:24.669057   17843 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I1110 14:55:24.669140   17843 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.005144254s
I1110 14:55:24.669272   17843 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I1110 14:55:24.669406   17843 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I1110 14:55:24.669550   17843 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I1110 14:55:24.669615   17843 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I1110 14:55:24.669685   17843 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 3.64146541s
I1110 14:55:24.669739   17843 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 6.330355166s
I1110 14:55:24.669791   17843 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 8.503725219s
I1110 14:55:24.669874   17843 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1110 14:55:24.669973   17843 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1110 14:55:24.670018   17843 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I1110 14:55:24.670158   17843 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1110 14:55:24.670201   17843 kubeadm.go:310] [bootstrap-token] Using token: c07re8.6exkm6onvpqwwd4i
I1110 14:55:24.706751   17843 out.go:252]     ‚ñ™ Configuring RBAC rules ...
I1110 14:55:24.706992   17843 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1110 14:55:24.707141   17843 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1110 14:55:24.707388   17843 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1110 14:55:24.707671   17843 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1110 14:55:24.707874   17843 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1110 14:55:24.708046   17843 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1110 14:55:24.708247   17843 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1110 14:55:24.708322   17843 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I1110 14:55:24.708381   17843 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I1110 14:55:24.708395   17843 kubeadm.go:310] 
I1110 14:55:24.708522   17843 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I1110 14:55:24.708527   17843 kubeadm.go:310] 
I1110 14:55:24.708725   17843 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I1110 14:55:24.708730   17843 kubeadm.go:310] 
I1110 14:55:24.708786   17843 kubeadm.go:310]   mkdir -p $HOME/.kube
I1110 14:55:24.708954   17843 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1110 14:55:24.709079   17843 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1110 14:55:24.709089   17843 kubeadm.go:310] 
I1110 14:55:24.709187   17843 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I1110 14:55:24.709193   17843 kubeadm.go:310] 
I1110 14:55:24.709376   17843 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1110 14:55:24.709382   17843 kubeadm.go:310] 
I1110 14:55:24.709475   17843 kubeadm.go:310] You should now deploy a pod network to the cluster.
I1110 14:55:24.709747   17843 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1110 14:55:24.709851   17843 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1110 14:55:24.709948   17843 kubeadm.go:310] 
I1110 14:55:24.710077   17843 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I1110 14:55:24.710207   17843 kubeadm.go:310] and service account keys on each node and then running the following as root:
I1110 14:55:24.710286   17843 kubeadm.go:310] 
I1110 14:55:24.710425   17843 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token c07re8.6exkm6onvpqwwd4i \
I1110 14:55:24.710589   17843 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:a36e5332befa8259dc1231f4bea98b633b9609ea716adc8d5ca91b21a83a3318 \
I1110 14:55:24.710619   17843 kubeadm.go:310] 	--control-plane 
I1110 14:55:24.710628   17843 kubeadm.go:310] 
I1110 14:55:24.710750   17843 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I1110 14:55:24.710758   17843 kubeadm.go:310] 
I1110 14:55:24.710872   17843 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token c07re8.6exkm6onvpqwwd4i \
I1110 14:55:24.711099   17843 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:a36e5332befa8259dc1231f4bea98b633b9609ea716adc8d5ca91b21a83a3318 
I1110 14:55:24.711123   17843 cni.go:84] Creating CNI manager for ""
I1110 14:55:24.711137   17843 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1110 14:55:24.738126   17843 out.go:179] üîó  Configuring bridge CNI (Container Networking Interface) ...
I1110 14:55:24.754667   17843 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1110 14:55:24.770347   17843 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I1110 14:55:24.801681   17843 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1110 14:55:24.803484   17843 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1110 14:55:24.804077   17843 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_11_10T14_55_24_0700 minikube.k8s.io/version=v1.37.0 minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I1110 14:55:24.824231   17843 ops.go:34] apiserver oom_adj: -16
I1110 14:55:25.008374   17843 kubeadm.go:1105] duration metric: took 205.074506ms to wait for elevateKubeSystemPrivileges
I1110 14:55:25.014749   17843 kubeadm.go:394] duration metric: took 16.642863536s to StartCluster
I1110 14:55:25.014772   17843 settings.go:142] acquiring lock: {Name:mk28e30c25437f45d719d571e39f828b11f1e13a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1110 14:55:25.014910   17843 settings.go:150] Updating kubeconfig:  /Users/lorenzotomo/.kube/config
I1110 14:55:25.018687   17843 lock.go:35] WriteFile acquiring /Users/lorenzotomo/.kube/config: {Name:mke4782a6892b14f121cd20b911d3fd4d625d753 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1110 14:55:25.019414   17843 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1110 14:55:25.019801   17843 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1110 14:55:25.019951   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1110 14:55:25.020083   17843 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1110 14:55:25.028938   17843 out.go:179] üîé  Verifying Kubernetes components...
I1110 14:55:25.028994   17843 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1110 14:55:25.029231   17843 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1110 14:55:25.039910   17843 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I1110 14:55:25.041572   17843 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1110 14:55:25.041851   17843 host.go:66] Checking if "minikube" exists ...
I1110 14:55:25.055404   17843 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1110 14:55:25.056710   17843 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1110 14:55:25.056861   17843 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1110 14:55:25.124618   17843 out.go:179]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1110 14:55:25.135590   17843 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1110 14:55:25.135605   17843 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1110 14:55:25.135742   17843 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 14:55:25.162827   17843 addons.go:238] Setting addon default-storageclass=true in "minikube"
I1110 14:55:25.162865   17843 host.go:66] Checking if "minikube" exists ...
I1110 14:55:25.164175   17843 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1110 14:55:25.195301   17843 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50356 SSHKeyPath:/Users/lorenzotomo/.minikube/machines/minikube/id_rsa Username:docker}
I1110 14:55:25.219336   17843 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I1110 14:55:25.219348   17843 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1110 14:55:25.219538   17843 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1110 14:55:25.269465   17843 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50356 SSHKeyPath:/Users/lorenzotomo/.minikube/machines/minikube/id_rsa Username:docker}
I1110 14:55:25.442052   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1110 14:55:25.466834   17843 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1110 14:55:25.552985   17843 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1110 14:55:25.716294   17843 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1110 14:55:26.241563   17843 start.go:976] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I1110 14:55:26.397989   17843 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1110 14:55:26.444302   17843 api_server.go:52] waiting for apiserver process to appear ...
I1110 14:55:26.444395   17843 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1110 14:55:26.458679   17843 api_server.go:72] duration metric: took 1.439069027s to wait for apiserver process to appear ...
I1110 14:55:26.458699   17843 api_server.go:88] waiting for apiserver healthz status ...
I1110 14:55:26.459308   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:55:31.250240   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": dial tcp: lookup minikube.local: no such host
I1110 14:55:31.250320   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
W1110 14:55:31.252301   17843 kapi.go:211] failed rescaling "coredns" deployment in "kube-system" namespace and "minikube" context to 1 replicas: non-retryable failure while getting "coredns" deployment scale: Get "https://minikube.local:50360/apis/apps/v1/namespaces/kube-system/deployments/coredns/scale": dial tcp: lookup minikube.local: no such host
E1110 14:55:31.252318   17843 start.go:160] Unable to scale down deployment "coredns" in namespace "kube-system" to 1 replica: non-retryable failure while getting "coredns" deployment scale: Get "https://minikube.local:50360/apis/apps/v1/namespaces/kube-system/deployments/coredns/scale": dial tcp: lookup minikube.local: no such host
W1110 14:55:31.252538   17843 out.go:285] ‚ùó  Enabling 'default-storageclass' returned an error: running callbacks: [Error making standard the default storage class: Error listing StorageClasses: Get "https://minikube.local:50360/apis/storage.k8s.io/v1/storageclasses": dial tcp: lookup minikube.local: no such host]
I1110 14:55:31.266782   17843 out.go:179] üåü  Enabled addons: storage-provisioner
I1110 14:55:31.291259   17843 addons.go:514] duration metric: took 6.271842775s for enable addons: enabled=[storage-provisioner]
I1110 14:55:36.252366   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": dial tcp: lookup minikube.local: no such host (Client.Timeout exceeded while awaiting headers)
I1110 14:55:36.252404   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:55:41.253666   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:55:41.253691   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:55:41.254402   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": dial tcp: lookup minikube.local: no such host
I1110 14:55:41.460185   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:55:46.461522   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:55:46.461615   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:55:46.462437   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": dial tcp: lookup minikube.local: no such host
I1110 14:55:46.960028   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:55:51.960944   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:55:51.960979   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:55:51.963751   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": dial tcp: lookup minikube.local: no such host
I1110 14:55:52.460239   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:55:57.464158   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:55:57.464215   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:56:02.465142   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:56:02.465238   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:56:02.468745   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": dial tcp: lookup minikube.local: no such host
I1110 14:56:02.960163   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:56:07.961455   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:56:07.961611   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:56:07.963222   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": dial tcp: lookup minikube.local: no such host
I1110 14:56:08.460506   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:56:13.461505   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:56:13.461541   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:56:13.463426   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": dial tcp: lookup minikube.local: no such host
I1110 14:56:13.960170   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:56:18.967780   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:56:18.967846   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:56:23.971651   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:56:23.971677   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:56:23.973699   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": dial tcp: lookup minikube.local: no such host
I1110 14:56:24.460698   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:56:29.461493   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:56:29.469853   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:56:29.546386   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:56:29.546498   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:56:29.579594   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:56:29.579744   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:56:29.604977   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:56:29.605095   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:56:29.641309   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:56:29.641531   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:56:29.662906   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:56:29.663059   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:56:29.691531   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:56:29.691681   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:56:29.713664   17843 logs.go:282] 0 containers: []
W1110 14:56:29.713677   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:56:29.713687   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:56:29.713698   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:56:29.776468   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:56:29.776482   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:56:29.798944   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:56:29.798961   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:56:29.847080   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:56:29.847093   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:56:29.878162   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:56:29.878175   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:56:29.901975   17843 logs.go:123] Gathering logs for container status ...
I1110 14:56:29.901987   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:56:30.008341   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:56:30.008353   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:56:30.130363   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:56:30.130375   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:56:30.175774   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:56:30.175789   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:56:30.216474   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:56:30.216486   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:56:30.245775   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:56:30.245787   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:56:30.272085   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:56:30.272112   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:56:32.812685   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:56:37.813586   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:56:37.814029   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:56:37.850172   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:56:37.850273   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:56:37.873161   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:56:37.873266   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:56:37.913059   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:56:37.913183   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:56:37.945606   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:56:37.945837   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:56:37.978493   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:56:37.978616   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:56:38.007024   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:56:38.007148   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:56:38.035264   17843 logs.go:282] 0 containers: []
W1110 14:56:38.035292   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:56:38.035301   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:56:38.035318   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:56:38.145730   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:56:38.145743   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:56:38.197666   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:56:38.197680   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:56:38.229377   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:56:38.229389   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:56:38.257299   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:56:38.257313   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:56:38.282311   17843 logs.go:123] Gathering logs for container status ...
I1110 14:56:38.282323   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:56:38.336584   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:56:38.336596   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:56:38.396601   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:56:38.396613   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:56:38.414719   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:56:38.414736   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:56:38.457250   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:56:38.457263   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:56:38.492522   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:56:38.492546   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:56:38.524569   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:56:38.524585   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:56:41.071313   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:56:46.072865   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:56:46.073224   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:56:46.100721   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:56:46.100824   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:56:46.123947   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:56:46.124085   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:56:46.150619   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:56:46.150754   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:56:46.173089   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:56:46.173257   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:56:46.197651   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:56:46.197808   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:56:46.225005   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:56:46.225169   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:56:46.245492   17843 logs.go:282] 0 containers: []
W1110 14:56:46.245507   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:56:46.245518   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:56:46.245528   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:56:46.272150   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:56:46.272171   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:56:46.297004   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:56:46.297017   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:56:46.353829   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:56:46.353853   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:56:46.371218   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:56:46.371230   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:56:46.473932   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:56:46.473944   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:56:46.554409   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:56:46.554428   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:56:46.596188   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:56:46.596199   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:56:46.648009   17843 logs.go:123] Gathering logs for container status ...
I1110 14:56:46.648026   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:56:46.713964   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:56:46.714001   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:56:46.758943   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:56:46.758966   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:56:46.790449   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:56:46.790460   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:56:49.323047   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:56:54.324379   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:56:54.324722   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:56:54.351330   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:56:54.351502   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:56:54.375711   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:56:54.375837   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:56:54.399816   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:56:54.399971   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:56:54.425213   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:56:54.425369   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:56:54.451593   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:56:54.451720   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:56:54.475153   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:56:54.475282   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:56:54.498172   17843 logs.go:282] 0 containers: []
W1110 14:56:54.498186   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:56:54.498196   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:56:54.498209   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:56:54.556185   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:56:54.556200   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:56:54.573471   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:56:54.573482   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:56:54.684209   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:56:54.684230   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:56:54.714149   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:56:54.714160   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:56:54.762650   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:56:54.762663   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:56:54.786170   17843 logs.go:123] Gathering logs for container status ...
I1110 14:56:54.786183   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:56:54.840622   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:56:54.840635   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:56:54.891732   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:56:54.891749   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:56:54.976579   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:56:54.976596   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:56:55.017407   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:56:55.017423   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:56:55.061169   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:56:55.061188   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:56:57.594155   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:57:02.595570   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:57:02.595867   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:57:02.621013   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:57:02.621127   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:57:02.643965   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:57:02.644071   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:57:02.669230   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:57:02.669355   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:57:02.701738   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:57:02.701837   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:57:02.724149   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:57:02.724287   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:57:02.746032   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:57:02.746159   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:57:02.771824   17843 logs.go:282] 0 containers: []
W1110 14:57:02.771838   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:57:02.771847   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:57:02.771858   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:57:02.812157   17843 logs.go:123] Gathering logs for container status ...
I1110 14:57:02.812171   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:57:02.860011   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:57:02.860023   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:57:02.911688   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:57:02.911703   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:57:02.966597   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:57:02.966609   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:57:02.996763   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:57:02.996776   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:57:03.021361   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:57:03.021373   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:57:03.054237   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:57:03.054259   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:57:03.078695   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:57:03.078706   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:57:03.095467   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:57:03.095479   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:57:03.212343   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:57:03.212360   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:57:03.254093   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:57:03.254108   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:57:05.782538   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:57:10.783662   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:57:10.784098   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:57:10.809977   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:57:10.810095   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:57:10.834653   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:57:10.834786   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:57:10.860338   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:57:10.860519   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:57:10.882299   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:57:10.882439   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:57:10.912097   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:57:10.912209   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:57:10.935584   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:57:10.935686   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:57:10.959917   17843 logs.go:282] 0 containers: []
W1110 14:57:10.959931   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:57:10.959939   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:57:10.959948   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:57:10.976668   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:57:10.976680   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:57:11.090251   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:57:11.090264   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:57:11.115551   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:57:11.115563   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:57:11.147250   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:57:11.147281   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:57:11.173337   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:57:11.173349   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:57:11.212881   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:57:11.212895   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:57:11.236022   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:57:11.236033   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:57:11.289088   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:57:11.289102   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:57:11.333567   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:57:11.333588   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:57:11.378608   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:57:11.378626   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:57:11.405548   17843 logs.go:123] Gathering logs for container status ...
I1110 14:57:11.405561   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:57:13.957261   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:57:18.958159   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:57:18.958343   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:57:18.983697   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:57:18.983801   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:57:19.019757   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:57:19.019890   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:57:19.048324   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:57:19.048494   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:57:19.076179   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:57:19.076285   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:57:19.105047   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:57:19.107922   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:57:19.134750   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:57:19.134888   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:57:19.157663   17843 logs.go:282] 0 containers: []
W1110 14:57:19.157675   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:57:19.157684   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:57:19.157692   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:57:19.181855   17843 logs.go:123] Gathering logs for container status ...
I1110 14:57:19.181868   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:57:19.233776   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:57:19.233788   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:57:19.346129   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:57:19.346149   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:57:19.382584   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:57:19.382605   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:57:19.411276   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:57:19.411290   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:57:19.447322   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:57:19.447334   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:57:19.496910   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:57:19.496926   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:57:19.552597   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:57:19.552642   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:57:19.573370   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:57:19.573385   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:57:19.626039   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:57:19.626053   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:57:19.654196   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:57:19.654208   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:57:22.193999   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:57:27.195341   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:57:27.195612   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:57:27.225892   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:57:27.225998   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:57:27.248150   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:57:27.248281   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:57:27.270919   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:57:27.271023   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:57:27.294416   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:57:27.294593   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:57:27.318641   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:57:27.318776   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:57:27.341608   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:57:27.341786   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:57:27.363541   17843 logs.go:282] 0 containers: []
W1110 14:57:27.363551   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:57:27.363559   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:57:27.363567   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:57:27.407736   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:57:27.407751   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:57:27.448444   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:57:27.448459   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:57:27.491085   17843 logs.go:123] Gathering logs for container status ...
I1110 14:57:27.491101   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:57:27.541380   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:57:27.541397   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:57:27.598587   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:57:27.598618   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:57:27.618252   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:57:27.618266   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:57:27.723885   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:57:27.723896   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:57:27.751937   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:57:27.751950   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:57:27.779699   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:57:27.779713   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:57:27.811771   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:57:27.811800   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:57:27.836542   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:57:27.836553   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:57:30.362124   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:57:35.362687   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:57:35.363148   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:57:35.587093   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:57:35.587261   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:57:35.628456   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:57:35.628599   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:57:35.689208   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:57:35.690901   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:57:35.725600   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:57:35.727165   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:57:35.776654   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:57:35.777143   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:57:35.816742   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:57:35.816891   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:57:35.939169   17843 logs.go:282] 0 containers: []
W1110 14:57:35.939183   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:57:35.939192   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:57:35.939201   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:57:36.002403   17843 logs.go:123] Gathering logs for container status ...
I1110 14:57:36.002416   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:57:36.087652   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:57:36.087667   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:57:36.113179   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:57:36.113191   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:57:36.183966   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:57:36.183988   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:57:36.219742   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:57:36.219757   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:57:36.252846   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:57:36.252859   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:57:36.300928   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:57:36.300951   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:57:36.335144   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:57:36.335178   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:57:36.408136   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:57:36.408153   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:57:36.537428   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:57:36.537444   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:57:36.595600   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:57:36.595620   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:57:39.133240   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:57:44.134031   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:57:44.134430   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:57:44.160939   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:57:44.161101   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:57:44.186395   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:57:44.186494   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:57:44.223337   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:57:44.223487   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:57:44.249118   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:57:44.249227   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:57:44.273052   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:57:44.273217   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:57:44.296558   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:57:44.296666   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:57:44.319390   17843 logs.go:282] 0 containers: []
W1110 14:57:44.319406   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:57:44.319414   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:57:44.319422   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:57:44.374764   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:57:44.374783   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:57:44.392179   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:57:44.392190   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:57:44.445499   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:57:44.445515   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:57:44.488472   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:57:44.488487   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:57:44.525213   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:57:44.525225   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:57:44.548682   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:57:44.548695   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:57:44.688482   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:57:44.688496   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:57:44.731151   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:57:44.731167   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:57:44.762354   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:57:44.762367   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:57:44.786986   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:57:44.786998   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:57:44.823684   17843 logs.go:123] Gathering logs for container status ...
I1110 14:57:44.823699   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:57:47.372166   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:57:52.373399   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:57:52.373708   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:57:52.402313   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:57:52.402468   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:57:52.428292   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:57:52.428408   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:57:52.452440   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:57:52.452552   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:57:52.472674   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:57:52.472798   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:57:52.501901   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:57:52.502020   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:57:52.523822   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:57:52.523944   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:57:52.545854   17843 logs.go:282] 0 containers: []
W1110 14:57:52.545866   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:57:52.545873   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:57:52.545881   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:57:52.569422   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:57:52.569434   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:57:52.611042   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:57:52.611057   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:57:52.637032   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:57:52.637045   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:57:52.663159   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:57:52.663171   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:57:52.692820   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:57:52.692832   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:57:52.717628   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:57:52.717640   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:57:52.758666   17843 logs.go:123] Gathering logs for container status ...
I1110 14:57:52.758681   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:57:52.811401   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:57:52.811412   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:57:52.865059   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:57:52.865133   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:57:52.881703   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:57:52.881722   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:57:52.985335   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:57:52.985346   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:57:55.539339   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:58:00.540255   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:58:00.540543   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:58:00.570071   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:58:00.570208   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:58:00.598533   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:58:00.598644   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:58:00.624318   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:58:00.624436   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:58:00.656464   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:58:00.656651   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:58:00.682509   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:58:00.682628   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:58:00.707739   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:58:00.707866   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:58:00.730364   17843 logs.go:282] 0 containers: []
W1110 14:58:00.730380   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:58:00.730392   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:58:00.730405   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:58:00.747401   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:58:00.747413   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:58:00.887878   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:58:00.887891   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:58:00.969598   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:58:00.969612   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:58:01.007681   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:58:01.007697   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:58:01.036607   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:58:01.036619   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:58:01.060220   17843 logs.go:123] Gathering logs for container status ...
I1110 14:58:01.060231   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:58:01.110893   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:58:01.110905   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:58:01.150034   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:58:01.150049   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:58:01.176511   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:58:01.176526   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:58:01.210099   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:58:01.210110   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:58:01.252345   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:58:01.252358   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:58:03.805514   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:58:08.806032   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:58:08.806250   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:58:08.833765   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:58:08.833884   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:58:08.854225   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:58:08.854336   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:58:08.888183   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:58:08.888311   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:58:08.911918   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:58:08.912035   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:58:08.940351   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:58:08.940518   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:58:08.965412   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:58:08.965558   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:58:08.990127   17843 logs.go:282] 0 containers: []
W1110 14:58:08.990148   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:58:08.990160   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:58:08.990170   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:58:09.025658   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:58:09.025671   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:58:09.071393   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:58:09.071415   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:58:09.098094   17843 logs.go:123] Gathering logs for container status ...
I1110 14:58:09.098109   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:58:09.156760   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:58:09.156772   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:58:09.213398   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:58:09.213412   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:58:09.231372   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:58:09.231391   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:58:09.340121   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:58:09.340132   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:58:09.383390   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:58:09.383405   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:58:09.428370   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:58:09.428386   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:58:09.453636   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:58:09.453648   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:58:09.482738   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:58:09.482758   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:58:12.011862   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:58:17.012827   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:58:17.013084   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:58:17.035250   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:58:17.035362   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:58:17.058435   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:58:17.058556   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:58:17.082218   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:58:17.082406   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:58:17.104935   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:58:17.105048   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:58:17.128104   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:58:17.128227   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:58:17.149983   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:58:17.150092   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:58:17.171328   17843 logs.go:282] 0 containers: []
W1110 14:58:17.171341   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:58:17.171349   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:58:17.171356   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:58:17.225994   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:58:17.226015   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:58:17.265104   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:58:17.265119   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:58:17.311236   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:58:17.311250   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:58:17.335512   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:58:17.335524   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:58:17.353658   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:58:17.353669   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:58:17.459614   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:58:17.459625   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:58:17.511190   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:58:17.511202   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:58:17.542985   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:58:17.542995   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:58:17.572141   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:58:17.572153   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:58:17.610936   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:58:17.610949   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:58:17.640430   17843 logs.go:123] Gathering logs for container status ...
I1110 14:58:17.640444   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:58:20.192640   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:58:25.194174   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:58:25.194513   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:58:25.228194   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:58:25.228307   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:58:25.253416   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:58:25.253572   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:58:25.277619   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:58:25.277754   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:58:25.310562   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:58:25.310731   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:58:25.337549   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:58:25.337655   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:58:25.359908   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:58:25.360079   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:58:25.383177   17843 logs.go:282] 0 containers: []
W1110 14:58:25.383192   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:58:25.383204   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:58:25.383212   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:58:25.441348   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:58:25.441363   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:58:25.459284   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:58:25.459296   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:58:25.563181   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:58:25.563192   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:58:25.610041   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:58:25.610056   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:58:25.637231   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:58:25.637242   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:58:25.669063   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:58:25.669075   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:58:25.711641   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:58:25.711657   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:58:25.750980   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:58:25.750993   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:58:25.779828   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:58:25.779840   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:58:25.811659   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:58:25.811670   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:58:25.837624   17843 logs.go:123] Gathering logs for container status ...
I1110 14:58:25.837636   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:58:28.388362   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:58:33.389796   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:58:33.389996   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:58:33.427072   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:58:33.427260   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:58:33.448698   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:58:33.448812   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:58:33.469992   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:58:33.472278   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:58:33.496034   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:58:33.496196   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:58:33.520960   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:58:33.521107   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:58:33.545226   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:58:33.545323   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:58:33.568546   17843 logs.go:282] 0 containers: []
W1110 14:58:33.568577   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:58:33.568587   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:58:33.568596   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:58:33.607517   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:58:33.607530   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:58:33.636368   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:58:33.636380   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:58:33.663248   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:58:33.663259   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:58:33.695444   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:58:33.695456   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:58:33.733226   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:58:33.733241   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:58:33.758290   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:58:33.758311   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:58:33.839032   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:58:33.839046   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:58:33.892204   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:58:33.892217   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:58:33.921610   17843 logs.go:123] Gathering logs for container status ...
I1110 14:58:33.921621   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:58:33.970151   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:58:33.970162   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:58:33.986574   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:58:33.986595   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:58:36.592381   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:58:41.593235   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:58:41.593490   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:58:41.624670   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:58:41.624815   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:58:41.660395   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:58:41.660510   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:58:41.691045   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:58:41.691200   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:58:41.760223   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:58:41.760405   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:58:41.784736   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:58:41.784911   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:58:41.805887   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:58:41.806039   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:58:41.830186   17843 logs.go:282] 0 containers: []
W1110 14:58:41.830232   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:58:41.830241   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:58:41.830262   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:58:41.877444   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:58:41.877468   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:58:41.905615   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:58:41.905634   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:58:41.950003   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:58:41.950015   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:58:41.976871   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:58:41.976888   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:58:42.084491   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:58:42.084511   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:58:42.126784   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:58:42.126798   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:58:42.171488   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:58:42.171507   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:58:42.214732   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:58:42.214744   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:58:42.244461   17843 logs.go:123] Gathering logs for container status ...
I1110 14:58:42.244479   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:58:42.292431   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:58:42.292451   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:58:42.343568   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:58:42.343578   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:58:44.860631   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:58:49.861388   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:58:49.861760   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:58:49.893040   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:58:49.893160   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:58:49.917195   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:58:49.917347   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:58:49.941817   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:58:49.942018   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:58:49.965159   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:58:49.965291   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:58:49.988720   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:58:49.988832   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:58:50.011773   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:58:50.011874   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:58:50.031491   17843 logs.go:282] 0 containers: []
W1110 14:58:50.031507   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:58:50.031516   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:58:50.031526   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:58:50.143682   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:58:50.143694   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:58:50.190246   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:58:50.190268   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:58:50.217251   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:58:50.217268   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:58:50.253064   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:58:50.253076   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:58:50.280831   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:58:50.280842   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:58:50.320909   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:58:50.320925   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:58:50.345851   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:58:50.345863   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:58:50.406090   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:58:50.406107   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:58:50.427482   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:58:50.427494   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:58:50.467263   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:58:50.467275   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:58:50.494073   17843 logs.go:123] Gathering logs for container status ...
I1110 14:58:50.494086   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:58:53.043589   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:58:58.044305   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:58:58.044528   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:58:58.071603   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:58:58.071730   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:58:58.101184   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:58:58.101321   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:58:58.129329   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:58:58.129470   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:58:58.154262   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:58:58.154389   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:58:58.180364   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:58:58.180494   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:58:58.207831   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:58:58.207961   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:58:58.234700   17843 logs.go:282] 0 containers: []
W1110 14:58:58.234713   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:58:58.234734   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:58:58.234747   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:58:58.261797   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:58:58.261811   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:58:58.282318   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:58:58.282339   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:58:58.390962   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:58:58.390975   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:58:58.421019   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:58:58.421038   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:58:58.452193   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:58:58.452207   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:58:58.489113   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:58:58.489126   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:58:58.517154   17843 logs.go:123] Gathering logs for container status ...
I1110 14:58:58.517177   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:58:58.571024   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:58:58.571036   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:58:58.626270   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:58:58.626285   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:58:58.671660   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:58:58.671673   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:58:58.711935   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:58:58.711948   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:59:01.253982   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:59:06.254961   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:59:06.255152   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:59:06.284187   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:59:06.284298   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:59:06.308983   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:59:06.309115   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:59:06.336572   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:59:06.336743   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:59:06.360401   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:59:06.360517   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:59:06.386109   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:59:06.386226   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:59:06.410546   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:59:06.410681   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:59:06.433430   17843 logs.go:282] 0 containers: []
W1110 14:59:06.433442   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:59:06.433451   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:59:06.433459   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:59:06.477739   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:59:06.477761   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:59:06.511821   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:59:06.511833   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:59:06.545824   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:59:06.545839   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:59:06.574917   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:59:06.574928   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:59:06.628257   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:59:06.628272   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:59:06.666080   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:59:06.666095   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:59:06.690987   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:59:06.690998   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:59:06.737623   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:59:06.737638   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:59:06.760758   17843 logs.go:123] Gathering logs for container status ...
I1110 14:59:06.760770   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:59:06.816089   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:59:06.816102   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:59:06.833283   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:59:06.833295   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:59:09.445732   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:59:14.447605   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:59:14.448386   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:59:14.473503   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:59:14.473599   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:59:14.496966   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:59:14.497151   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:59:14.520225   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:59:14.520355   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:59:14.542122   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:59:14.542249   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:59:14.575534   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:59:14.575770   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:59:14.606650   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:59:14.606777   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:59:14.628500   17843 logs.go:282] 0 containers: []
W1110 14:59:14.628534   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:59:14.628544   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:59:14.628552   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:59:14.658872   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:59:14.658903   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:59:14.682738   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:59:14.682755   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:59:14.719075   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:59:14.719088   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:59:14.747839   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:59:14.747860   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:59:14.778020   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:59:14.778046   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:59:14.825521   17843 logs.go:123] Gathering logs for container status ...
I1110 14:59:14.825538   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:59:14.884227   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:59:14.884250   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:59:14.968920   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:59:14.968944   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:59:15.010148   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:59:15.010163   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:59:15.143687   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:59:15.143699   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:59:15.189062   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:59:15.189092   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:59:17.719463   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:59:22.720655   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:59:22.721091   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1110 14:59:22.747075   17843 logs.go:282] 1 containers: [07a572985a22]
I1110 14:59:22.747198   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1110 14:59:22.771169   17843 logs.go:282] 1 containers: [6e61a031c052]
I1110 14:59:22.771289   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1110 14:59:22.792846   17843 logs.go:282] 2 containers: [ddd3b9c1a7e2 e317232e3f0f]
I1110 14:59:22.792990   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1110 14:59:22.815528   17843 logs.go:282] 1 containers: [2e94f573f493]
I1110 14:59:22.815650   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1110 14:59:22.839360   17843 logs.go:282] 1 containers: [095978313cee]
I1110 14:59:22.839471   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1110 14:59:22.863505   17843 logs.go:282] 1 containers: [08458b9cd9f5]
I1110 14:59:22.863625   17843 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1110 14:59:22.885631   17843 logs.go:282] 0 containers: []
W1110 14:59:22.885642   17843 logs.go:284] No container was found matching "kindnet"
I1110 14:59:22.885650   17843 logs.go:123] Gathering logs for kube-proxy [095978313cee] ...
I1110 14:59:22.885658   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 095978313cee"
I1110 14:59:22.914610   17843 logs.go:123] Gathering logs for Docker ...
I1110 14:59:22.914623   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1110 14:59:22.940423   17843 logs.go:123] Gathering logs for kube-apiserver [07a572985a22] ...
I1110 14:59:22.940436   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 07a572985a22"
I1110 14:59:22.995392   17843 logs.go:123] Gathering logs for etcd [6e61a031c052] ...
I1110 14:59:22.995405   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6e61a031c052"
I1110 14:59:23.039128   17843 logs.go:123] Gathering logs for coredns [ddd3b9c1a7e2] ...
I1110 14:59:23.039148   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddd3b9c1a7e2"
I1110 14:59:23.066455   17843 logs.go:123] Gathering logs for kube-scheduler [2e94f573f493] ...
I1110 14:59:23.066467   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2e94f573f493"
I1110 14:59:23.102217   17843 logs.go:123] Gathering logs for kube-controller-manager [08458b9cd9f5] ...
I1110 14:59:23.102233   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 08458b9cd9f5"
I1110 14:59:23.153148   17843 logs.go:123] Gathering logs for container status ...
I1110 14:59:23.153170   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1110 14:59:23.206750   17843 logs.go:123] Gathering logs for kubelet ...
I1110 14:59:23.206761   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1110 14:59:23.263720   17843 logs.go:123] Gathering logs for dmesg ...
I1110 14:59:23.263733   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1110 14:59:23.283244   17843 logs.go:123] Gathering logs for describe nodes ...
I1110 14:59:23.283257   17843 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1110 14:59:23.398082   17843 logs.go:123] Gathering logs for coredns [e317232e3f0f] ...
I1110 14:59:23.398094   17843 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e317232e3f0f"
I1110 14:59:25.929254   17843 api_server.go:253] Checking apiserver healthz at https://minikube.local:50360/healthz ...
I1110 14:59:30.930303   17843 api_server.go:269] stopped: https://minikube.local:50360/healthz: Get "https://minikube.local:50360/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1110 14:59:30.947960   17843 out.go:203] 
W1110 14:59:30.963157   17843 out.go:285] ‚ùå  Exiting due to GUEST_START: failed to start node: wait 6m0s for node: wait for healthy API server: apiserver healthz never reported healthy: context deadline exceeded
W1110 14:59:30.963194   17843 out.go:285] 
W1110 14:59:30.993298   17843 out.go:308] [31m‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    üòø  If the above advice does not help, please let us know:                             [31m‚îÇ[0m
[31m‚îÇ[0m    üëâ  https://github.com/kubernetes/minikube/issues/new/choose                           [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚îÇ[0m    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                           [31m‚îÇ[0m
[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ[0m
I1110 14:59:31.016454   17843 out.go:203] 


==> Docker <==
Nov 10 13:55:00 minikube dockerd[656]: time="2025-11-10T13:55:00.987501663Z" level=info msg="Daemon has completed initialization"
Nov 10 13:55:00 minikube dockerd[656]: time="2025-11-10T13:55:00.987686118Z" level=info msg="API listen on /run/docker.sock"
Nov 10 13:55:00 minikube dockerd[656]: time="2025-11-10T13:55:00.987736305Z" level=info msg="API listen on /var/run/docker.sock"
Nov 10 13:55:00 minikube dockerd[656]: time="2025-11-10T13:55:00.987844238Z" level=info msg="API listen on [::]:2376"
Nov 10 13:55:00 minikube systemd[1]: Started Docker Application Container Engine.
Nov 10 13:55:02 minikube dockerd[656]: time="2025-11-10T13:55:02.253145545Z" level=error msg="Failed to get event" error="rpc error: code = Unavailable desc = error reading from server: EOF" module=libcontainerd namespace=plugins.moby
Nov 10 13:55:02 minikube dockerd[656]: time="2025-11-10T13:55:02.253180420Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=plugins.moby
Nov 10 13:55:02 minikube dockerd[656]: time="2025-11-10T13:55:02.254370791Z" level=error msg="Failed to get event" error="rpc error: code = Unavailable desc = error reading from server: EOF" module=libcontainerd namespace=moby
Nov 10 13:55:02 minikube dockerd[656]: time="2025-11-10T13:55:02.254393135Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=moby
Nov 10 13:55:02 minikube dockerd[656]: time="2025-11-10T13:55:02.414543244Z" level=error msg="Failed to get event" error="rpc error: code = Unavailable desc = error reading from server: EOF" module=libcontainerd namespace=moby
Nov 10 13:55:02 minikube dockerd[656]: time="2025-11-10T13:55:02.414576572Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=moby
Nov 10 13:55:02 minikube dockerd[656]: time="2025-11-10T13:55:02.415351756Z" level=error msg="Failed to get event" error="rpc error: code = Unavailable desc = error reading from server: EOF" module=libcontainerd namespace=plugins.moby
Nov 10 13:55:02 minikube dockerd[656]: time="2025-11-10T13:55:02.415414155Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=plugins.moby
Nov 10 13:55:02 minikube systemd[1]: Stopping Docker Application Container Engine...
Nov 10 13:55:02 minikube dockerd[656]: time="2025-11-10T13:55:02.851634533Z" level=info msg="Processing signal 'terminated'"
Nov 10 13:55:02 minikube dockerd[656]: time="2025-11-10T13:55:02.854097405Z" level=warning msg="Error while testing if containerd API is ready" error="Canceled: grpc: the client connection is closing"
Nov 10 13:55:02 minikube dockerd[656]: time="2025-11-10T13:55:02.854606596Z" level=info msg="Daemon shutdown complete"
Nov 10 13:55:02 minikube systemd[1]: docker.service: Deactivated successfully.
Nov 10 13:55:02 minikube systemd[1]: Stopped Docker Application Container Engine.
Nov 10 13:55:02 minikube systemd[1]: docker.service: Consumed 1.170s CPU time.
Nov 10 13:55:02 minikube systemd[1]: Starting Docker Application Container Engine...
Nov 10 13:55:03 minikube dockerd[1089]: time="2025-11-10T13:55:03.010626605Z" level=info msg="Starting up"
Nov 10 13:55:03 minikube dockerd[1089]: time="2025-11-10T13:55:03.012452180Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Nov 10 13:55:03 minikube dockerd[1089]: time="2025-11-10T13:55:03.012634528Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/var/run/cdi
Nov 10 13:55:03 minikube dockerd[1089]: time="2025-11-10T13:55:03.012650474Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/etc/cdi
Nov 10 13:55:03 minikube dockerd[1089]: time="2025-11-10T13:55:03.030433849Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Nov 10 13:55:03 minikube dockerd[1089]: time="2025-11-10T13:55:03.036552013Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Nov 10 13:55:03 minikube dockerd[1089]: time="2025-11-10T13:55:03.059392740Z" level=info msg="Loading containers: start."
Nov 10 13:55:04 minikube dockerd[1089]: time="2025-11-10T13:55:04.064198564Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count d5c6af786d4965a90f7fb9166567ddb17d58ab9a470f65d785578fd438ac57bc], retrying...."
Nov 10 13:55:04 minikube dockerd[1089]: time="2025-11-10T13:55:04.147088572Z" level=info msg="Loading containers: done."
Nov 10 13:55:04 minikube dockerd[1089]: time="2025-11-10T13:55:04.170462965Z" level=info msg="Docker daemon" commit=249d679 containerd-snapshotter=false storage-driver=overlay2 version=28.4.0
Nov 10 13:55:04 minikube dockerd[1089]: time="2025-11-10T13:55:04.170569221Z" level=info msg="Initializing buildkit"
Nov 10 13:55:04 minikube dockerd[1089]: time="2025-11-10T13:55:04.206265604Z" level=info msg="Completed buildkit initialization"
Nov 10 13:55:04 minikube dockerd[1089]: time="2025-11-10T13:55:04.217364471Z" level=info msg="Daemon has completed initialization"
Nov 10 13:55:04 minikube dockerd[1089]: time="2025-11-10T13:55:04.217471715Z" level=info msg="API listen on /run/docker.sock"
Nov 10 13:55:04 minikube dockerd[1089]: time="2025-11-10T13:55:04.217533903Z" level=info msg="API listen on [::]:2376"
Nov 10 13:55:04 minikube dockerd[1089]: time="2025-11-10T13:55:04.217564485Z" level=info msg="API listen on /var/run/docker.sock"
Nov 10 13:55:04 minikube systemd[1]: Started Docker Application Container Engine.
Nov 10 13:55:04 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Nov 10 13:55:05 minikube cri-dockerd[1397]: time="2025-11-10T13:55:05Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Nov 10 13:55:05 minikube cri-dockerd[1397]: time="2025-11-10T13:55:05Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Nov 10 13:55:05 minikube cri-dockerd[1397]: time="2025-11-10T13:55:05Z" level=info msg="Start docker client with request timeout 0s"
Nov 10 13:55:05 minikube cri-dockerd[1397]: time="2025-11-10T13:55:05Z" level=info msg="Hairpin mode is set to hairpin-veth"
Nov 10 13:55:05 minikube cri-dockerd[1397]: time="2025-11-10T13:55:05Z" level=info msg="Loaded network plugin cni"
Nov 10 13:55:05 minikube cri-dockerd[1397]: time="2025-11-10T13:55:05Z" level=info msg="Docker cri networking managed by network plugin cni"
Nov 10 13:55:05 minikube cri-dockerd[1397]: time="2025-11-10T13:55:05Z" level=info msg="Setting cgroupDriver cgroupfs"
Nov 10 13:55:05 minikube cri-dockerd[1397]: time="2025-11-10T13:55:05Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Nov 10 13:55:05 minikube cri-dockerd[1397]: time="2025-11-10T13:55:05Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Nov 10 13:55:05 minikube cri-dockerd[1397]: time="2025-11-10T13:55:05Z" level=info msg="Start cri-dockerd grpc backend"
Nov 10 13:55:05 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Nov 10 13:55:15 minikube cri-dockerd[1397]: time="2025-11-10T13:55:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ff773c098b91c8f1b11e8cca4832c49d03f5f1b7b0b71d9c34201aad3ba049d2/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 10 13:55:15 minikube cri-dockerd[1397]: time="2025-11-10T13:55:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ac5366c9e526a85c149fb91cb33a76c0eedadb3729e8a0daedc66a2ace802b0f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 10 13:55:15 minikube cri-dockerd[1397]: time="2025-11-10T13:55:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/65843dd897f9039b3eb552908bb6b11c8d8c17ffd6b92f96df55ab53f5786c6c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 10 13:55:15 minikube cri-dockerd[1397]: time="2025-11-10T13:55:15Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7dd387d0fdaf36e3778c1e78ebfa2c1e41aacaee8446c396f2537c0909188860/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 10 13:55:30 minikube cri-dockerd[1397]: time="2025-11-10T13:55:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7d2e1b0725af9fb25017078e99553819bc45da2afa6e786cde36e0e365e97c21/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 10 13:55:30 minikube cri-dockerd[1397]: time="2025-11-10T13:55:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d3bcade5c5bb66fbea1ce8bd1e13d5aa87f478253e7ea6721165a3ffa48de2c9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 10 13:55:30 minikube cri-dockerd[1397]: time="2025-11-10T13:55:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/df7b12d412736f3df3a7a2bd26980b179c586fa83bbd0eb6cb00f467f62aef40/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 10 13:55:30 minikube cri-dockerd[1397]: time="2025-11-10T13:55:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d3c9a1129161eb3b4c29d97bd4c9dcf1acd7636324c1c8fc2194a345355cd610/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Nov 10 13:55:34 minikube cri-dockerd[1397]: time="2025-11-10T13:55:34Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Nov 10 13:56:00 minikube dockerd[1089]: time="2025-11-10T13:56:00.306539468Z" level=info msg="ignoring event" container=5a056a45e3726e9122c6527b528517397ec86ffd58bb89ab18eb199d65162180 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
f6b55c7c90753       6e38f40d628db       4 minutes ago       Running             storage-provisioner       1                   7d2e1b0725af9       storage-provisioner
ddd3b9c1a7e24       52546a367cc9e       4 minutes ago       Running             coredns                   0                   df7b12d412736       coredns-66bc5c9577-xvwr9
e317232e3f0f8       52546a367cc9e       4 minutes ago       Running             coredns                   0                   d3c9a1129161e       coredns-66bc5c9577-49rpg
095978313ceec       df0860106674d       4 minutes ago       Running             kube-proxy                0                   d3bcade5c5bb6       kube-proxy-wvvcj
5a056a45e3726       6e38f40d628db       4 minutes ago       Exited              storage-provisioner       0                   7d2e1b0725af9       storage-provisioner
6e61a031c0527       5f1f5298c888d       5 minutes ago       Running             etcd                      0                   7dd387d0fdaf3       etcd-minikube
2e94f573f4939       46169d968e920       5 minutes ago       Running             kube-scheduler            0                   65843dd897f90       kube-scheduler-minikube
07a572985a227       90550c43ad2bc       5 minutes ago       Running             kube-apiserver            0                   ac5366c9e526a       kube-apiserver-minikube
08458b9cd9f5b       a0af72f2ec6d6       5 minutes ago       Running             kube-controller-manager   0                   ff773c098b91c       kube-controller-manager-minikube


==> coredns [ddd3b9c1a7e2] <==
maxprocs: Leaving GOMAXPROCS=4: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:36733 - 5323 "HINFO IN 3530623170069737562.7943451748261268881. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.052359675s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error


==> coredns [e317232e3f0f] <==
maxprocs: Leaving GOMAXPROCS=4: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:50884 - 49469 "HINFO IN 8780557712937322379.6600557987637808453. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.046619739s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_11_10T14_55_24_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 10 Nov 2025 13:55:20 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 10 Nov 2025 14:00:18 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 10 Nov 2025 13:59:59 +0000   Mon, 10 Nov 2025 13:55:17 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 10 Nov 2025 13:59:59 +0000   Mon, 10 Nov 2025 13:55:17 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 10 Nov 2025 13:59:59 +0000   Mon, 10 Nov 2025 13:55:17 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 10 Nov 2025 13:59:59 +0000   Mon, 10 Nov 2025 13:55:21 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  239856220Ki
  hugepages-2Mi:      0
  memory:             4008188Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  239856220Ki
  hugepages-2Mi:      0
  memory:             4008188Ki
  pods:               110
System Info:
  Machine ID:                 86c4f8018032407fa57bb8502eb34cfa
  System UUID:                86c4f8018032407fa57bb8502eb34cfa
  Boot ID:                    c21beb11-4673-4997-a531-aaec30283fbc
  Kernel Version:             6.10.14-linuxkit
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-66bc5c9577-49rpg            100m (2%)     0 (0%)      70Mi (1%)        170Mi (4%)     4m57s
  kube-system                 coredns-66bc5c9577-xvwr9            100m (2%)     0 (0%)      70Mi (1%)        170Mi (4%)     4m57s
  kube-system                 etcd-minikube                       100m (2%)     0 (0%)      100Mi (2%)       0 (0%)         5m3s
  kube-system                 kube-apiserver-minikube             250m (6%)     0 (0%)      0 (0%)           0 (0%)         5m3s
  kube-system                 kube-controller-manager-minikube    200m (5%)     0 (0%)      0 (0%)           0 (0%)         5m5s
  kube-system                 kube-proxy-wvvcj                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m58s
  kube-system                 kube-scheduler-minikube             100m (2%)     0 (0%)      0 (0%)           0 (0%)         5m3s
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m1s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (21%)  0 (0%)
  memory             240Mi (6%)  340Mi (8%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 4m55s                  kube-proxy       
  Normal  Starting                 5m13s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  5m13s (x8 over 5m13s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    5m13s (x8 over 5m13s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     5m13s (x7 over 5m13s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  5m13s                  kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 5m4s                   kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  5m3s                   kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  5m3s                   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    5m3s                   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     5m3s                   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           4m59s                  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Nov10 13:19] RETBleed: WARNING: Spectre v2 mitigation leaves CPU vulnerable to RETBleed attacks, data leaks possible!
[  +0.457641] pci 0000:00:1f.0: BAR 7: [io  size 0x0080] has bogus alignment
[  +0.021812] virtio-pci 0000:00:01.0: can't derive routing for PCI INT A
[  +0.000001] virtio-pci 0000:00:01.0: PCI INT A: no GSI
[  +0.003211] virtio-pci 0000:00:05.0: can't derive routing for PCI INT A
[  +0.000002] virtio-pci 0000:00:05.0: PCI INT A: no GSI
[  +0.003094] virtio-pci 0000:00:06.0: can't derive routing for PCI INT A
[  +0.000001] virtio-pci 0000:00:06.0: PCI INT A: no GSI
[  +0.003063] virtio-pci 0000:00:07.0: can't derive routing for PCI INT A
[  +0.000001] virtio-pci 0000:00:07.0: PCI INT A: no GSI
[  +0.002848] virtio-pci 0000:00:08.0: can't derive routing for PCI INT A
[  +0.000001] virtio-pci 0000:00:08.0: PCI INT A: no GSI
[  +0.005257] virtio-pci 0000:00:09.0: can't derive routing for PCI INT A
[  +0.000002] virtio-pci 0000:00:09.0: PCI INT A: no GSI
[  +0.005211] virtio-pci 0000:00:0a.0: can't derive routing for PCI INT A
[  +0.000002] virtio-pci 0000:00:0a.0: PCI INT A: no GSI
[  +0.005365] virtio-pci 0000:00:0b.0: can't derive routing for PCI INT A
[  +0.000002] virtio-pci 0000:00:0b.0: PCI INT A: no GSI
[  +0.002824] virtio-pci 0000:00:0c.0: can't derive routing for PCI INT A
[  +0.000002] virtio-pci 0000:00:0c.0: PCI INT A: no GSI
[  +0.002537] virtio-pci 0000:00:0d.0: can't derive routing for PCI INT A
[  +0.000002] virtio-pci 0000:00:0d.0: PCI INT A: no GSI
[  +0.000632] virtio-pci 0000:00:0e.0: can't derive routing for PCI INT A
[  +0.000001] virtio-pci 0000:00:0e.0: PCI INT A: no GSI
[  +0.000615] virtio-pci 0000:00:0f.0: can't derive routing for PCI INT A
[  +0.000001] virtio-pci 0000:00:0f.0: PCI INT A: no GSI
[  +0.015389] Hangcheck: starting hangcheck timer 0.9.1 (tick is 180 seconds, margin is 60 seconds).
[  +0.071111] lpc_ich 0000:00:1f.0: No MFD cells added
[  +0.545662] netlink: 'init': attribute type 4 has an invalid length.
[  +0.230451] fakeowner: loading out-of-tree module taints kernel.


==> etcd [6e61a031c052] <==
{"level":"warn","ts":"2025-11-10T13:55:18.530685Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40178","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.546416Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40186","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.560548Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40228","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.622743Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40240","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.632733Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40256","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.642675Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40274","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.653141Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40294","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.660877Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40322","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.673244Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40332","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.712816Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40358","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.724620Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40370","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.739492Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40390","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.754513Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40418","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.762673Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40438","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.775261Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40462","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.814934Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40468","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.825427Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40488","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.840942Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40506","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.850092Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40524","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.860805Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40554","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.947614Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40576","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.957318Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40604","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.967705Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40610","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:18.982821Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40626","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.011683Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40648","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.021140Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40656","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.034719Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40682","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.048749Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40694","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.059991Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40718","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.074387Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40740","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.107832Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40750","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.117777Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40770","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.131323Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40798","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.154867Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40836","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.164163Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40844","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.178879Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40860","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.309926Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40876","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.321412Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40880","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.339680Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40896","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.409948Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40916","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.423339Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40940","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.441059Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40944","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.453977Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40958","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.462593Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40964","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.472493Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40972","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.510016Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40998","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.514909Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41018","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.524360Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41036","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.538399Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41052","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.551029Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41064","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.587573Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41076","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.593813Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41104","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.607633Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41132","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.617534Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41146","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.626693Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41162","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.646197Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41168","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.658522Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41182","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.668334Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41196","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.680433Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41216","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-10T13:55:19.770222Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:51944","server-name":"","error":"EOF"}


==> kernel <==
 14:00:27 up 41 min,  0 users,  load average: 2.04, 1.98, 2.01
Linux minikube 6.10.14-linuxkit #1 SMP PREEMPT_DYNAMIC Wed Sep 10 06:48:35 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [07a572985a22] <==
I1110 13:55:20.911634       1 controller.go:90] Starting OpenAPI V3 controller
I1110 13:55:20.911808       1 naming_controller.go:299] Starting NamingConditionController
I1110 13:55:20.911974       1 establishing_controller.go:81] Starting EstablishingController
I1110 13:55:20.912310       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1110 13:55:20.912441       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1110 13:55:20.912529       1 crd_finalizer.go:269] Starting CRDFinalizer
I1110 13:55:20.810041       1 shared_informer.go:349] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I1110 13:55:20.912952       1 repairip.go:210] Starting ipallocator-repair-controller
I1110 13:55:20.913036       1 shared_informer.go:349] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I1110 13:55:20.809692       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1110 13:55:20.916481       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1110 13:55:20.925560       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1110 13:55:20.925680       1 policy_source.go:240] refreshing policies
I1110 13:55:21.011311       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I1110 13:55:21.013261       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I1110 13:55:21.015634       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I1110 13:55:21.017319       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1110 13:55:21.017378       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1110 13:55:21.017550       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1110 13:55:21.017805       1 aggregator.go:171] initial CRD sync complete...
I1110 13:55:21.017946       1 autoregister_controller.go:144] Starting autoregister controller
I1110 13:55:21.017958       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1110 13:55:21.017966       1 cache.go:39] Caches are synced for autoregister controller
I1110 13:55:21.019825       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I1110 13:55:21.023727       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I1110 13:55:21.025490       1 cache.go:39] Caches are synced for LocalAvailability controller
I1110 13:55:21.025510       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I1110 13:55:21.027889       1 default_servicecidr_controller.go:166] Creating default ServiceCIDR with CIDRs: [10.96.0.0/12]
I1110 13:55:21.050109       1 controller.go:667] quota admission added evaluator for: namespaces
E1110 13:55:21.125052       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I1110 13:55:21.155040       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I1110 13:55:21.222715       1 default_servicecidr_controller.go:228] Setting default ServiceCIDR condition Ready to True
I1110 13:55:21.248718       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1110 13:55:21.249643       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1110 13:55:21.252610       1 default_servicecidr_controller.go:228] Setting default ServiceCIDR condition Ready to True
I1110 13:55:21.329886       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I1110 13:55:21.826406       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1110 13:55:21.832569       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1110 13:55:21.832608       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1110 13:55:22.767091       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1110 13:55:22.895308       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1110 13:55:23.045901       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W1110 13:55:23.054319       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1110 13:55:23.055674       1 controller.go:667] quota admission added evaluator for: endpoints
I1110 13:55:23.066858       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1110 13:55:23.956617       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I1110 13:55:24.047617       1 controller.go:667] quota admission added evaluator for: deployments.apps
I1110 13:55:24.060345       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I1110 13:55:24.119301       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I1110 13:55:29.546324       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1110 13:55:29.555027       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1110 13:55:29.941697       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I1110 13:55:29.990111       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I1110 13:56:35.757601       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1110 13:56:39.979751       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1110 13:57:44.836107       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1110 13:57:55.373906       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1110 13:59:09.962698       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1110 13:59:13.668001       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1110 14:00:23.134497       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [08458b9cd9f5] <==
I1110 13:55:28.839817       1 controllermanager.go:781] "Started controller" controller="persistentvolume-expander-controller"
I1110 13:55:28.839873       1 controllermanager.go:733] "Controller is disabled by a feature gate" controller="storageversion-garbage-collector-controller" requiredFeatureGates=["APIServerIdentity","StorageVersionAPI"]
I1110 13:55:28.839889       1 expand_controller.go:327] "Starting expand controller" logger="persistentvolume-expander-controller"
I1110 13:55:28.839904       1 shared_informer.go:349] "Waiting for caches to sync" controller="expand"
I1110 13:55:28.844854       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1110 13:55:28.862151       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1110 13:55:28.872120       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1110 13:55:28.887746       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1110 13:55:28.893573       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1110 13:55:28.894821       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1110 13:55:28.894944       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1110 13:55:28.909544       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1110 13:55:28.917592       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1110 13:55:28.919751       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1110 13:55:28.920454       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1110 13:55:28.922438       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1110 13:55:28.924500       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1110 13:55:28.924604       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1110 13:55:28.924658       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1110 13:55:28.924722       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1110 13:55:28.924830       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1110 13:55:28.928504       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1110 13:55:28.932813       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1110 13:55:28.933041       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1110 13:55:28.933365       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1110 13:55:28.933648       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1110 13:55:28.933734       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1110 13:55:28.937275       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1110 13:55:28.937988       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1110 13:55:28.938740       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1110 13:55:28.939035       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1110 13:55:28.939081       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1110 13:55:28.939874       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1110 13:55:28.940257       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1110 13:55:28.938038       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1110 13:55:28.941143       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1110 13:55:28.938022       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1110 13:55:28.941811       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1110 13:55:28.942385       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1110 13:55:28.943516       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1110 13:55:28.944030       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1110 13:55:28.945303       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1110 13:55:28.946727       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1110 13:55:28.952449       1 shared_informer.go:356] "Caches are synced" controller="node"
I1110 13:55:28.952567       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1110 13:55:28.952597       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1110 13:55:28.952603       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1110 13:55:28.952608       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1110 13:55:28.962776       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1110 13:55:28.963492       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I1110 13:55:28.972682       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1110 13:55:28.975761       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1110 13:55:28.986217       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1110 13:55:28.986378       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1110 13:55:28.986395       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1110 13:55:28.989844       1 shared_informer.go:356] "Caches are synced" controller="job"
I1110 13:55:28.990385       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1110 13:55:28.991584       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1110 13:55:28.991770       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1110 13:55:28.991851       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"


==> kube-proxy [095978313cee] <==
I1110 13:55:30.949589       1 server_linux.go:53] "Using iptables proxy"
I1110 13:55:31.080935       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1110 13:55:31.181275       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1110 13:55:31.181340       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1110 13:55:31.181452       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1110 13:55:31.332863       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1110 13:55:31.332964       1 server_linux.go:132] "Using iptables Proxier"
I1110 13:55:31.369044       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1110 13:55:31.369629       1 server.go:527] "Version info" version="v1.34.0"
I1110 13:55:31.369668       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1110 13:55:31.372626       1 config.go:200] "Starting service config controller"
I1110 13:55:31.372693       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1110 13:55:31.372736       1 config.go:106] "Starting endpoint slice config controller"
I1110 13:55:31.372762       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1110 13:55:31.372810       1 config.go:403] "Starting serviceCIDR config controller"
I1110 13:55:31.372840       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1110 13:55:31.377567       1 config.go:309] "Starting node config controller"
I1110 13:55:31.377591       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1110 13:55:31.473475       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1110 13:55:31.473470       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1110 13:55:31.473506       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1110 13:55:31.480011       1 shared_informer.go:356] "Caches are synced" controller="node config"


==> kube-scheduler [2e94f573f493] <==
I1110 13:55:18.019334       1 serving.go:386] Generated self-signed cert in-memory
W1110 13:55:20.921522       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1110 13:55:20.922106       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1110 13:55:20.922325       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1110 13:55:20.922393       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1110 13:55:21.024363       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1110 13:55:21.024426       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1110 13:55:21.035091       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1110 13:55:21.035172       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1110 13:55:21.036241       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1110 13:55:21.036562       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E1110 13:55:21.046170       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1110 13:55:21.058080       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1110 13:55:21.058185       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1110 13:55:21.109010       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1110 13:55:21.110348       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1110 13:55:21.110386       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1110 13:55:21.110669       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1110 13:55:21.110765       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1110 13:55:21.110836       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1110 13:55:21.110914       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1110 13:55:21.110985       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1110 13:55:21.111052       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1110 13:55:21.111132       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1110 13:55:21.111195       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1110 13:55:21.111257       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1110 13:55:21.111341       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1110 13:55:21.111443       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1110 13:55:21.112737       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1110 13:55:21.112874       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1110 13:55:21.918787       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1110 13:55:21.925165       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1110 13:55:21.961833       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1110 13:55:22.011231       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1110 13:55:22.100959       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1110 13:55:22.151666       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1110 13:55:22.201091       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1110 13:55:22.317505       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1110 13:55:22.334595       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1110 13:55:22.380874       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1110 13:55:22.381033       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1110 13:55:22.384312       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1110 13:55:22.384416       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1110 13:55:22.387792       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1110 13:55:22.422040       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1110 13:55:22.440825       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
I1110 13:55:24.536735       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Nov 10 13:55:24 minikube kubelet[2242]: E1110 13:55:24.069971    2242 manager.go:513] "Failed to read data from checkpoint" err="checkpoint is not found" checkpoint="kubelet_internal_checkpoint"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.070751    2242 eviction_manager.go:189] "Eviction manager: starting control loop"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.070805    2242 container_log_manager.go:146] "Initializing container log rotate workers" workers=1 monitorPeriod="10s"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.108951    2242 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Nov 10 13:55:24 minikube kubelet[2242]: E1110 13:55:24.114477    2242 eviction_manager.go:267] "eviction manager: failed to check if we have separate container filesystem. Ignoring." err="no imagefs label for configured runtime"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.142666    2242 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.144106    2242 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: E1110 13:55:24.158697    2242 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.168355    2242 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.211223    2242 kubelet_node_status.go:75] "Attempting to register node" node="minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.237693    2242 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.240349    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.240416    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.240441    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.240463    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.240502    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/e3a36fac0ae701bc11fad0a6716eec2c-etcd-certs\") pod \"etcd-minikube\" (UID: \"e3a36fac0ae701bc11fad0a6716eec2c\") " pod="kube-system/etcd-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.240531    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/8312b4cdc4b705c0e12f63794469cfad-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"8312b4cdc4b705c0e12f63794469cfad\") " pod="kube-system/kube-apiserver-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.240565    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/8312b4cdc4b705c0e12f63794469cfad-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"8312b4cdc4b705c0e12f63794469cfad\") " pod="kube-system/kube-apiserver-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.240587    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.240613    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/dc6cf0a7bcb54d1f95cecc4d7b6b7d67-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"dc6cf0a7bcb54d1f95cecc4d7b6b7d67\") " pod="kube-system/kube-scheduler-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.240637    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/8312b4cdc4b705c0e12f63794469cfad-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"8312b4cdc4b705c0e12f63794469cfad\") " pod="kube-system/kube-apiserver-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.240747    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.242455    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.243040    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/e3a36fac0ae701bc11fad0a6716eec2c-etcd-data\") pod \"etcd-minikube\" (UID: \"e3a36fac0ae701bc11fad0a6716eec2c\") " pod="kube-system/etcd-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.243132    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/8312b4cdc4b705c0e12f63794469cfad-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"8312b4cdc4b705c0e12f63794469cfad\") " pod="kube-system/kube-apiserver-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.243179    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/8312b4cdc4b705c0e12f63794469cfad-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"8312b4cdc4b705c0e12f63794469cfad\") " pod="kube-system/kube-apiserver-minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.256627    2242 kubelet_node_status.go:124] "Node was previously registered" node="minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.256935    2242 kubelet_node_status.go:78] "Successfully registered node" node="minikube"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.912391    2242 apiserver.go:52] "Watching apiserver"
Nov 10 13:55:24 minikube kubelet[2242]: I1110 13:55:24.939560    2242 desired_state_of_world_populator.go:154] "Finished populating initial desired state of world"
Nov 10 13:55:25 minikube kubelet[2242]: I1110 13:55:25.318757    2242 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Nov 10 13:55:25 minikube kubelet[2242]: I1110 13:55:25.320275    2242 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Nov 10 13:55:25 minikube kubelet[2242]: E1110 13:55:25.351619    2242 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Nov 10 13:55:25 minikube kubelet[2242]: E1110 13:55:25.354026    2242 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Nov 10 13:55:25 minikube kubelet[2242]: I1110 13:55:25.445292    2242 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=3.445268881 podStartE2EDuration="3.445268881s" podCreationTimestamp="2025-11-10 13:55:22 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-11-10 13:55:25.40921445 +0000 UTC m=+1.612210067" watchObservedRunningTime="2025-11-10 13:55:25.445268881 +0000 UTC m=+1.648264450"
Nov 10 13:55:25 minikube kubelet[2242]: I1110 13:55:25.518046    2242 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=1.518025833 podStartE2EDuration="1.518025833s" podCreationTimestamp="2025-11-10 13:55:24 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-11-10 13:55:25.515915923 +0000 UTC m=+1.718911522" watchObservedRunningTime="2025-11-10 13:55:25.518025833 +0000 UTC m=+1.721021404"
Nov 10 13:55:25 minikube kubelet[2242]: I1110 13:55:25.518228    2242 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=1.518221473 podStartE2EDuration="1.518221473s" podCreationTimestamp="2025-11-10 13:55:24 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-11-10 13:55:25.445764554 +0000 UTC m=+1.648760129" watchObservedRunningTime="2025-11-10 13:55:25.518221473 +0000 UTC m=+1.721217046"
Nov 10 13:55:25 minikube kubelet[2242]: I1110 13:55:25.541465    2242 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=1.541438942 podStartE2EDuration="1.541438942s" podCreationTimestamp="2025-11-10 13:55:24 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-11-10 13:55:25.5398511 +0000 UTC m=+1.742846692" watchObservedRunningTime="2025-11-10 13:55:25.541438942 +0000 UTC m=+1.744434518"
Nov 10 13:55:29 minikube kubelet[2242]: I1110 13:55:29.053660    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/fac4f109-5b7a-4ca7-856f-65b2ff6791bb-tmp\") pod \"storage-provisioner\" (UID: \"fac4f109-5b7a-4ca7-856f-65b2ff6791bb\") " pod="kube-system/storage-provisioner"
Nov 10 13:55:29 minikube kubelet[2242]: I1110 13:55:29.053845    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mhfb9\" (UniqueName: \"kubernetes.io/projected/fac4f109-5b7a-4ca7-856f-65b2ff6791bb-kube-api-access-mhfb9\") pod \"storage-provisioner\" (UID: \"fac4f109-5b7a-4ca7-856f-65b2ff6791bb\") " pod="kube-system/storage-provisioner"
Nov 10 13:55:29 minikube kubelet[2242]: E1110 13:55:29.162280    2242 projected.go:291] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Nov 10 13:55:29 minikube kubelet[2242]: E1110 13:55:29.162344    2242 projected.go:196] Error preparing data for projected volume kube-api-access-mhfb9 for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Nov 10 13:55:29 minikube kubelet[2242]: E1110 13:55:29.162499    2242 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/fac4f109-5b7a-4ca7-856f-65b2ff6791bb-kube-api-access-mhfb9 podName:fac4f109-5b7a-4ca7-856f-65b2ff6791bb nodeName:}" failed. No retries permitted until 2025-11-10 13:55:29.662464008 +0000 UTC m=+5.865459577 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-mhfb9" (UniqueName: "kubernetes.io/projected/fac4f109-5b7a-4ca7-856f-65b2ff6791bb-kube-api-access-mhfb9") pod "storage-provisioner" (UID: "fac4f109-5b7a-4ca7-856f-65b2ff6791bb") : configmap "kube-root-ca.crt" not found
Nov 10 13:55:30 minikube kubelet[2242]: I1110 13:55:30.165513    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/0ff19be6-51aa-4c88-88cc-db720e40e9a6-lib-modules\") pod \"kube-proxy-wvvcj\" (UID: \"0ff19be6-51aa-4c88-88cc-db720e40e9a6\") " pod="kube-system/kube-proxy-wvvcj"
Nov 10 13:55:30 minikube kubelet[2242]: I1110 13:55:30.165606    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-v9n5b\" (UniqueName: \"kubernetes.io/projected/0ff19be6-51aa-4c88-88cc-db720e40e9a6-kube-api-access-v9n5b\") pod \"kube-proxy-wvvcj\" (UID: \"0ff19be6-51aa-4c88-88cc-db720e40e9a6\") " pod="kube-system/kube-proxy-wvvcj"
Nov 10 13:55:30 minikube kubelet[2242]: I1110 13:55:30.165638    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/0ff19be6-51aa-4c88-88cc-db720e40e9a6-kube-proxy\") pod \"kube-proxy-wvvcj\" (UID: \"0ff19be6-51aa-4c88-88cc-db720e40e9a6\") " pod="kube-system/kube-proxy-wvvcj"
Nov 10 13:55:30 minikube kubelet[2242]: I1110 13:55:30.165661    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/0ff19be6-51aa-4c88-88cc-db720e40e9a6-xtables-lock\") pod \"kube-proxy-wvvcj\" (UID: \"0ff19be6-51aa-4c88-88cc-db720e40e9a6\") " pod="kube-system/kube-proxy-wvvcj"
Nov 10 13:55:30 minikube kubelet[2242]: I1110 13:55:30.266615    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/d893b641-78ac-4b51-a930-0b79a5578c78-config-volume\") pod \"coredns-66bc5c9577-49rpg\" (UID: \"d893b641-78ac-4b51-a930-0b79a5578c78\") " pod="kube-system/coredns-66bc5c9577-49rpg"
Nov 10 13:55:30 minikube kubelet[2242]: I1110 13:55:30.266691    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-xglk2\" (UniqueName: \"kubernetes.io/projected/e1ddee24-e3d5-4e24-bdcf-f7d9b39e375b-kube-api-access-xglk2\") pod \"coredns-66bc5c9577-xvwr9\" (UID: \"e1ddee24-e3d5-4e24-bdcf-f7d9b39e375b\") " pod="kube-system/coredns-66bc5c9577-xvwr9"
Nov 10 13:55:30 minikube kubelet[2242]: I1110 13:55:30.266806    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6n9dt\" (UniqueName: \"kubernetes.io/projected/d893b641-78ac-4b51-a930-0b79a5578c78-kube-api-access-6n9dt\") pod \"coredns-66bc5c9577-49rpg\" (UID: \"d893b641-78ac-4b51-a930-0b79a5578c78\") " pod="kube-system/coredns-66bc5c9577-49rpg"
Nov 10 13:55:30 minikube kubelet[2242]: I1110 13:55:30.266837    2242 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/e1ddee24-e3d5-4e24-bdcf-f7d9b39e375b-config-volume\") pod \"coredns-66bc5c9577-xvwr9\" (UID: \"e1ddee24-e3d5-4e24-bdcf-f7d9b39e375b\") " pod="kube-system/coredns-66bc5c9577-xvwr9"
Nov 10 13:55:31 minikube kubelet[2242]: I1110 13:55:31.437153    2242 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=5.437138752 podStartE2EDuration="5.437138752s" podCreationTimestamp="2025-11-10 13:55:26 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-11-10 13:55:30.37948806 +0000 UTC m=+6.582483654" watchObservedRunningTime="2025-11-10 13:55:31.437138752 +0000 UTC m=+7.640134326"
Nov 10 13:55:31 minikube kubelet[2242]: I1110 13:55:31.453569    2242 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-wvvcj" podStartSLOduration=2.453531619 podStartE2EDuration="2.453531619s" podCreationTimestamp="2025-11-10 13:55:29 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-11-10 13:55:31.441315791 +0000 UTC m=+7.644311366" watchObservedRunningTime="2025-11-10 13:55:31.453531619 +0000 UTC m=+7.656527189"
Nov 10 13:55:31 minikube kubelet[2242]: I1110 13:55:31.476655    2242 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-66bc5c9577-49rpg" podStartSLOduration=1.47664132 podStartE2EDuration="1.47664132s" podCreationTimestamp="2025-11-10 13:55:30 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-11-10 13:55:31.456552971 +0000 UTC m=+7.659548554" watchObservedRunningTime="2025-11-10 13:55:31.47664132 +0000 UTC m=+7.679636889"
Nov 10 13:55:32 minikube kubelet[2242]: I1110 13:55:32.188240    2242 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-66bc5c9577-xvwr9" podStartSLOduration=2.188126368 podStartE2EDuration="2.188126368s" podCreationTimestamp="2025-11-10 13:55:30 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-11-10 13:55:31.477454589 +0000 UTC m=+7.680450182" watchObservedRunningTime="2025-11-10 13:55:32.188126368 +0000 UTC m=+8.391121942"
Nov 10 13:55:34 minikube kubelet[2242]: I1110 13:55:34.339124    2242 kuberuntime_manager.go:1828] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Nov 10 13:55:34 minikube kubelet[2242]: I1110 13:55:34.340808    2242 kubelet_network.go:47] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Nov 10 13:55:38 minikube kubelet[2242]: I1110 13:55:38.691478    2242 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Nov 10 13:55:39 minikube kubelet[2242]: I1110 13:55:39.011593    2242 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Nov 10 13:56:00 minikube kubelet[2242]: I1110 13:56:00.726444    2242 scope.go:117] "RemoveContainer" containerID="5a056a45e3726e9122c6527b528517397ec86ffd58bb89ab18eb199d65162180"

